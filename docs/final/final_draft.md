Executive Summary 
Project Overview 
PolicyPilot emerged from a critical observation about the American healthcare system: nearly nineteen percent of in-network health insurance claims are denied, yet less than one-tenth of one percent of those denials are ever appealed by patients. This vast disparity reveals a systemic failure where patients who have valid grounds for appeal simply give up due to the overwhelming complexity, time burden, and lack of knowledge required to navigate the appeals process. Even more striking is that when appeals are actually submitted, approximately 60% succeed in overturning the denial, demonstrating that many denials are indeed unjustified or correctable. 
PolicyPilot addresses this gap by providing an artificial intelligence-powered platform that guides patients through the health insurance denial appeal process. The application acts as an intelligent partner that analyzes policy documents, interprets denial letters, identifies weaknesses in insurance company reasoning, and drafts professional appeal correspondence. Critically, the system maintains human oversight at every step, requiring user review and approval before any action is taken. This design philosophy emerged directly from user research revealing that patients reject fully automated solutions for high-stakes healthcare decisions, instead desiring an educational tool that empowers them while reducing the knowledge and time barriers that currently prevent successful appeals. 
Our Solution 
The implemented PolicyPilot platform represents a carefully architected system that balances AI capabilities with user agency and trust. At its core, the application consists of three primary components working in concert: a React-based frontend providing an intuitive user interface, a Node.js backend managing data and orchestrating AI operations, and a Python-based Retrieval-Augmented Generation pipeline that performs document analysis and generates appeal strategies. 
Users begin their journey by adding their insurance plan information to the system, uploading policy documents that are then processed and indexed for future reference. When a claim denial occurs, users create a new case, upload their denial letters and related medical bills, and the system extracts key information such as the denial reason and relevant policy details. The RAG pipeline analyzes these documents by chunking the text, generating embeddings using the HuggingFace all-MiniLM-L6-v2 model, and storing them in a ChromaDB vector database for efficient retrieval. When generating analysis or appeal drafts, the system queries this vector store to find relevant policy sections and denial details, then uses Google's Gemini large language model to generate human-readable explanations and professionally crafted appeal letters  
A significant architectural decision was the integration with Gmail API to handle email correspondence on behalf of users. Rather than having users manually copy and paste appeal letters, PolicyPilot sends emails directly from a dedicated agent account (policypilotco@gmail.com), applies labels to track conversations by user, and monitors incoming responses. When insurance companies reply, the system analyzes the response using the same RAG pipeline, identifying weaknesses in the insurer's arguments and preparing context-aware follow-up drafts for user approval. 
Key Outcomes 
PolicyPilot successfully demonstrates that artificial intelligence can meaningfully assist patients in navigating complex bureaucratic processes without removing human agency. The platform reduces the time required to prepare an appeal from multiple hours of research and drafting to approximately fifteen minutes of guided interaction and review. By automatically extracting relevant policy sections and citing specific coverage language in generated appeals, the system provides patients with professional-quality correspondence that would otherwise require hiring legal assistance. 
The modular architecture positions PolicyPilot for future expansion into adjacent problem spaces, such as proactive claim submission assistance, automated denial monitoring, and integration with healthcare provider billing systems. The technical foundation built around vector search, LLM orchestration, and secure document processing creates reusable patterns applicable to other domains where individuals face information asymmetry against large institutions. 
Major Pivots and Milestones 
The development journey involved several critical pivots that shaped the final product. Initially, the team envisioned users sending appeal emails directly from their personal email accounts, with PolicyPilot monitoring their inbox for responses. However, this approach raised significant privacy concerns about accessing user inboxes, even with OAuth consent. The pivot to using a dedicated PolicyPilot email account that acts as an advocate on the user's behalf solved the privacy issue while actually improving the user experience by completely automating the send-and-monitor workflow. 
Another important evolution was in the onboarding flow for insurance plans. The original design required users to upload their policy documents every time they wanted to create a new appeal case. User testing revealed this to be unnecessarily burdensome, especially for families managing multiple claims under the same insurance plan throughout the year. The revised architecture separated insurance plan management from case creation, allowing users to add their policy information once and then quickly initiate new appeal cases by simply selecting which plan and covered individual the case pertains to. 
The technical infrastructure underwent significant refinement as well. The initial prototype stored PDF files directly in MongoDB as binary data, which imposed strict size limitations. Integration with Supabase storage buckets enabled support for large policy documents exceeding one hundred megabytes while maintaining MongoDB for metadata and case management. This hybrid storage approach provided the best of both worlds: Supabase's robust file storage for large documents and MongoDB's flexible schema for rapidly evolving case data structures. 
Key milestones throughout development included discovering through user research that patients fundamentally reject black-box automation for healthcare decisions, successfully implementing the RAG pipeline with vector similarity search and LLM-generated analysis, integrating the Gmail API for bidirectional email automation, fine-tuning document embeddings to improve retrieval relevance, developing an email analysis agent capable of identifying weaknesses in insurance adjuster responses, and deploying a fully functional MVP with actual user-generated test cases validating the end-to-end workflow. 
 
Problem Definition & Background Research 
Problem Statement 
The fundamental problem PolicyPilot addresses sits at the intersection of healthcare access, administrative complexity, and systemic power imbalances. When insured patients in the United States receive medical services, they reasonably expect their insurance company to cover the costs according to their policy terms. However, a significant portion of legitimate claims are denied, not necessarily due to valid coverage exclusions, but due to administrative errors, ambiguous policy language, or deliberate cost-containment strategies by insurers. 
The crisis deepens when examining appeal rates. With denial rates approaching nineteen percent for Affordable Care Act marketplace plans, one would expect a proportional volume of appeals as patients challenge these decisions. Instead, the appeal rate languishes below one-tenth of one percent. This ninety-nine-point-nine percent non-appeal rate cannot be explained by patients agreeing with denial decisions. Rather, it reflects a system designed to be confusing and burdensome enough that most patients simply give up and either pay out of pocket or forego necessary medical care entirely. 
Recent investigations have exposed how this dynamic has intensified with the introduction of artificial intelligence in claims processing by insurance companies. A lawsuit against Cigna revealed the company allegedly using AI to mass-deny three hundred thousand claims within a two-month period, with only two-tenths of one percent of affected patients appealing these algorithmic denials (from: problem_def_background.md). The irony is stark: insurers deploy AI to scale denials, while patients lack the tools to scale effective responses. 
Our understanding of this problem evolved significantly through user research. Initially, we framed the issue purely as one of complexity and time burden. Interviews revealed a more nuanced reality. The problem is not simply that the process is confusing, though it certainly is. Nor is it merely time-consuming, though it absolutely demands hours of work. The core issue is a knowledge gap combined with an emotional toll. Patients describe feeling betrayed by a system they pay into monthly, powerless against large institutional actors, and uncertain whether fighting is even worth the effort when the financial and emotional costs of appealing might exceed the denied amount. 
The evolution in our problem definition led to a critical realization that shaped the entire product: patients do not simply want automation, they want empowerment. They need a tool that makes them informed participants in their own advocacy, not a black box that takes over. This insight fundamentally influenced our design philosophy toward transparency, education, and maintained user control. 
Background Research 
The broader market context reveals a landscape where existing solutions fail to adequately serve individual patients. Nonprofit advocacy organizations like Patient Advocate Foundation provide free case management services, but they are severely understaffed, serving hundreds of thousands of patients annually with only a couple hundred staff members. Patients report waiting months to receive assistance, by which time appeal deadlines may have passed. These organizations demonstrate demand exists but lack the scalability to meet it. 
On the commercial side, the denial management market is growing rapidly, but almost exclusively focuses on healthcare providers rather than patients. Companies like Aspirion, Cofactor AI, Elion Health, and H2O.ai have built AI-powered denial management systems, but their customers are hospitals and medical practices seeking to maximize reimbursement rates, not individual patients trying to get coverage for their own care. This provider-centric approach leaves a gap in the consumer market that PolicyPilot aims to fill. 
The increasing digitalization of healthcare infrastructure creates both challenges and opportunities. Insurance companies have begun requiring electronic submission of appeals and prior authorization requests, moving away from fax and mail. While this modernization could streamline processes, it currently adds another technical barrier for patients unfamiliar with portal navigation and digital form submission. 78% of physician offices and ninety-six percent of non-federal hospitals now use electronic health records systems, and insurers like UnitedHealthcare mandate electronic submissions for Medicaid claims in certain states. This digital transformation creates an opportune moment for PolicyPilot's web-based approach, meeting patients where the system is heading rather than where it has been. 
Recent regulatory and investigative attention on algorithmic claim denials adds urgency to the problem. A United States Senate investigation into UnitedHealth revealed that denial rates for Medicare Advantage patients seeking post-acute care skyrocketed from one-point-four percent in 2019 to twelve-point-six percent in 2022, resulting in over thirty-four thousand denied claims. Investigators found evidence suggesting AI-driven decision-making led to premature care terminations for vulnerable elderly patients recovering from strokes, falls, and injuries. When the very companies using AI to scale denials face minimal pushback because patients lack tools to respond at equivalent scale, the power imbalance becomes untenable. 
Changes Since Initial Proposal 
Our initial understanding of the problem centered heavily on the procedural complexity of appeals. We believed that if we could simply automate the paperwork and provide templates, patients would successfully challenge denials. The first pivot came from recognizing that procedural guidance alone was insufficient. Patients needed more than a step-by-step checklist; they needed actual analysis of their specific policy language matched against their specific denial reason. This insight led to adopting the RAG architecture for document-specific reasoning rather than generic template generation. 
The second major shift involved understanding the timeline and lifecycle of denials. Initially, we assumed all denials followed the same formal multi-stage appeal process. Research revealed that many denials are actually resolved through simple claims corrections involving direct communication with insurance adjusters rather than formal appeals paperwork. This discovery led to prioritizing email-based correspondence for the MVP rather than complex appeal form generation, significantly simplifying the initial implementation while addressing the majority of use cases. 
Perhaps the most profound change involved recognizing the emotional and trust dimensions of the problem. User research made clear that patients experiencing claim denials are not in a rational, calm state. They are often simultaneously dealing with health crises, financial stress, and feelings of betrayal by their insurance company. Any solution must acknowledge this emotional context and provide not just functional capability but also empathy and transparency. This realization influenced everything from the tone of UI copy to the architectural decision to maintain approval gates at every step, ensuring users never feel their control has been usurped by the system. 
User Research & Requirements 
Research Methodology 
The PolicyPilot team designed a comprehensive customer discovery approach targeting both direct stakeholders affected by claim denials and industry professionals who understand the system's inner workings. The research strategy employed multiple channels to ensure demographic and experiential diversity among participants. 
The primary outreach mechanism involved engaging with caregiver-focused Facebook groups, where the team searched for keywords such as "claim denied" and "appeal" to identify individuals actively experiencing denial-related frustration. These online communities provided access to the target demographic of middle-aged adults managing healthcare for themselves and family members. The team complemented this approach with personal referrals from healthcare contacts who could introduce them to both patients and insurance industry insiders. Additionally, LinkedIn cold outreach targeted professionals with titles like "patient advocate" and "healthcare administrator" to gain systems-level insights into denial patterns and procedural bottlenecks. 
A particularly valuable strategy involved partnering with local clinic billing managers and patient advocates. These frontline professionals see denial struggles daily and provided both individual patient referrals and aggregate insights about common failure patterns. One key informant, Reza, the CEO of Serenity MSO, a mental health clinic with a dedicated advocacy office, offered extensive perspective on both out-of-network and in-network denial dynamics. 
The data collection methodology combined structured interviews with validation surveys. An initial pre-screening survey identified candidates with significant denial experience suitable for in-depth interviews. The team then conducted thirty-minute interviews exploring participants' stories, frustrations, and motivations. Following interviews, a validation survey quantified whether pain points heard anecdotally were widespread across a larger sample. This mixed-methods approach balanced the rich narrative detail of interviews with the statistical validation of surveys. 
Interview questions progressed from open-ended context setting through deep dives into specific frustrations, concluding with ideal-world scenarios and AI trust questions. The protocol asked participants to walk through their most recent denial experience in detail, estimating time spent and identifying the most frustrating aspects. Interviewers probed on what resources participants turned to for help and what motivated them to either persist with an appeal or abandon it. The closing questions explicitly explored comfort with AI assistance, including concerns about privacy, accuracy, and level of human oversight desired. 
Key Research Findings 
The research uncovered four critical insights that fundamentally shaped PolicyPilot's design and feature prioritization. First, the appeals process is an overwhelming maze of fragmented information and unclear pathways. Participants described receiving multiple documents from different sources following a denied claim: an Explanation of Benefits from the insurer, itemized bills from the hospital or provider, and potentially multiple bills from different physicians or departments involved in a single episode of care. Jennifer, a thirty-nine-year-old woman who appealed a Level 4 emergency room visit denial, captured this chaos by describing how it took significant time just to "fully understand what I was actually being billed" as she pieced together overlapping and sometimes contradictory paperwork. 
The communication channels add further complexity. Andy, a twenty-four-year-old covered under his father's union-based insurance plan, found that determining whom to contact was itself a major barrier. The insurance company redirected him to the union representative, who in turn claimed he wasn't properly enrolled, creating a circular loop of bureaucratic confusion. He summarized his primary frustration as "understanding who you needed to talk to" and navigating a communication chain between Cigna, the union, and healthcare providers that felt deliberately inefficient. 
This maze-like quality extends beyond mere document volume to encompass the mindset required. Jennifer explained that her previous work at a medical clinic handling off-label drug appeals gave her a crucial advantage: she learned "not just to take health insurance and the bills at face value." Most people lack this learned skepticism and procedural knowledge, accepting initial denials as final decisions because they trust the system to work properly. After all, as Jennifer noted, "you pay all this money for it, so you assume that when you go, it's gonna work properly". 
Second, winning appeals requires knowledge that most people overlook or simply don't possess. The research revealed a consistent pattern: successful appellants possessed specific knowledge about their rights and policy details that unsuccessful appellants lacked. This knowledge gap operates on multiple levels. 
At the most basic level, many people are unaware that appeals are even possible. Multiple interviewees mentioned knowing few people who had attempted appeals, suggesting a fundamental gap in understanding patient rights. Those who do attempt appeals often don't realize that insurance companies themselves have procedural obligations and deadlines. Jennifer revealed a crucial piece of insider knowledge: if an insurance company or government agency misses its deadline to respond to an appeal, the claim might automatically be approved. However, this "loophole" is one "very few consumers know to track or enforce". 
Even when patients attempt appeals, they often lack the language and structure to craft persuasive arguments. Mike, a sixty-three-year-old managing his mother's care, succeeded in reducing an eighteen-thousand-dollar bill to two hundred eighty dollars by knowing to first request an itemized bill from the hospital (which immediately revealed erroneous charges) and then submitting a clear list of precise questions to the insurance adjuster about each uncovered procedure. His background in insurance gave him this procedural knowledge that most patients don't have. 
The most striking knowledge gap involves locating and interpreting the insurance policy itself. Reza, the clinic CEO with extensive appeal experience, made a provocative claim: he would "bet any amount of money that most adults couldn't locate their insurance policy if asked." Even if they found it, the policy document typically spans hundreds of pages of dense legal language that only an attorney could fully interpret. Insurance companies deliberately write policies to be inaccessible, knowing this opacity favors them when disputes arise (from: user_research.md, Reza interview summary). 
Third, the cost-benefit analysis of appealing often favors giving up. Multiple participants described making explicit calculations where the financial and emotional costs of continuing to fight exceeded the potential recovery amount, even when they believed their appeal had merit. Jennifer's case exemplifies this dynamic. Her denied emergency room visit for a potential blood clot cost her approximately three thousand dollars. While she found evidence through online research that UnitedHealthcare had a known pattern of denying Level 4 emergency visits, she ultimately concluded "it didn't make sense to go through a more intense appeal process and involve lawyers because I would have spent more" on legal fees than the denied amount. This rational economic decision meant the insurance company's questionable denial stood unchallenged, and they faced no accountability. 
The calculation isn't purely financial. Participants consistently mentioned the emotional drain of fighting bureaucracies while simultaneously managing health issues. The time burden compounds with other responsibilities. The target demographic of middle-aged adults in their forties through sixties are often managing medical needs for children, aging parents, and themselves simultaneously, alongside work and household responsibilities. Every hour spent navigating insurance appeals is an hour not spent on caregiving, employment, or self-care. 
Reza provided the provider perspective on this dynamic. When insurance companies underpay in-network providers, the doctor faces a similar calculation: hiring someone to fight for a small difference in payment often costs more than the underpayment itself. "This is why insurance companies get away with shortchanging providers" - they rely on the rational decision to give up when the fight costs more than the prize (from: user_research.md, Reza interview quote). 
Fourth, and perhaps most critically, patients need transparency and control, not a black-box solution. This finding directly contradicted the team's initial assumption that full automation would be the most desirable feature. Andy, despite being tech-savvy and regularly using AI tools like ChatGPT for schoolwork, emphatically stated he would not use an AI app that fully automated insurance appeals. His primary concern was privacy - connecting his insurance and hospital accounts to a third-party app felt like too much data exposure. But equally important was his need for human empathy in the process. He wanted to see messaging like "so sorry you had to go through this, I have resolved it for you" rather than cold algorithmic processing of his healthcare crisis. 
Jennifer, a technology professional who understands AI's capabilities and limitations, said she would only use an AI appeals app "with strong guardrails." She specifically demanded "the ability to review and approve any action before it's submitted" as essential, non-negotiable functionality. Her trust threshold for AI is conditional on maintaining final decision authority, especially for matters involving healthcare and finances. 
This pattern held across all interviews. Participants wanted assistance, not replacement. They wanted to become informed, empowered participants in their own advocacy rather than passive recipients of automated service. The desire for transparency extended to understanding the reasoning behind recommendations. Users wanted to see which specific policy sections supported their appeal, why the AI suggested a particular approach, and how confidence levels varied across different strategy options. 
Updated User Needs and Pain Points 
Functional Requirements: 
The system must enable secure upload and storage of policy documents and denial letters in PDF and image formats. Users need the ability to upload multiple policy documents when they lack a single comprehensive policy file, as is common with employer-sponsored plans that provide separate Summary of Benefits and Explanation of Benefits documents. The document ingestion pipeline must parse these files to extract key identifiers including insurance company name, policy number, and stated denial reasons, handling variations in document formatting across different insurers. 
The platform must generate plain-language explanations of why claims were denied, translating insurance jargon into accessible English. This translation capability needs to identify hard technical terms that appear in the analysis and provide hover-tooltip definitions so users can learn the vocabulary rather than remaining confused. The system should determine whether a case requires a simple claims correction or a formal multi-stage appeal, with the MVP focusing on the claims correction pathway since research showed this resolves over half of denials. 
For claims corrections, the system must draft initial outreach emails that cite specific policy sections supporting the user's position, include appropriate subject lines and correct recipient contact information, and allow user editing before sending. After users approve correspondence, the platform must send emails from a dedicated PolicyPilot account, monitor for insurer responses, and when replies arrive, provide clarifying explanations of any confusing insurance adjuster arguments along with context-aware follow-up drafts for user review. 
The system must provide a clear dashboard showing all active and resolved cases, allowing users to add multiple insurance plans with covered individuals specified for each plan, and enabling quick initiation of new appeals by selecting the relevant plan and person. Users must be able to view complete email threads with analysis overlays highlighting key terms and weak arguments from insurers. 
Non-Functional Requirements: 
Performance requirements dictate that document analysis should complete within two minutes for typical policy and denial documents, with progressive loading indicators keeping users informed of progress. The email drafting process should produce initial drafts within thirty seconds of analysis completion. The user interface must remain responsive with less than two hundred milliseconds of perceived latency for navigation and basic interactions. 
Reliability requirements specify that the system must successfully extract denial reasons from at least ninety percent of uploaded documents, with fallback to manual entry when automatic extraction fails. Generated appeal drafts should cite accurate policy sections with at least eighty percent precision, avoiding hallucinated references. The email sending and monitoring system must reliably detect incoming responses without missing reply threads. 
Scalability considerations acknowledge that the MVP targets individual users and small families rather than enterprise-scale usage. The architecture must support multiple concurrent users but need not optimize for thousands of simultaneous document uploads. Future iterations may require horizontal scaling of the RAG pipeline and LLM inference, but the initial implementation focuses on proving value with a manageable user base. 
Security and privacy requirements demand HIPAA compliance throughout the system. All uploaded documents must be encrypted at rest and in transit. Personal health information must be de-identified before processing by AI models where possible. Users must explicitly consent to HIPAA authorization and terms of service before accessing functionality. The system must maintain audit logs of all data access and processing operations. 
How User Insights Informed Pivots 
The user research directly shaped three major architectural and design pivots that define the current PolicyPilot implementation. The first pivot involved shifting from user-email sending to agent-based email management. Initially, the team planned to have PolicyPilot compose emails that users would send from their own personal email addresses, with the platform monitoring their inbox for responses via OAuth integration. Andy's privacy concerns about granting inbox access and Jennifer's emphasis on strong guardrails made clear this approach would face trust barriers. The solution of having PolicyPilot send emails from a dedicated agent account (policypilotco@gmail.com) on the user's behalf actually increased trust by limiting data exposure while still automating the send-and-monitor workflow. This pivot eliminated the privacy concern while delivering on the time-saving benefit users desired. 
The second pivot separated insurance plan management from case creation workflows. Early prototypes required users to upload policy documents each time they initiated an appeal. Mike's interview revealed he managed multiple claims for his mother throughout the year under the same insurance plan. The revised design where users add insurance plans once, specify covered individuals, and then create cases by simply selecting which plan and person the case pertains to dramatically reduces repeated effort for users managing multiple claims. This change directly addressed the time burden pain point while making the platform more practical for family caregivers managing care for multiple people. 
The third pivot involved the level of detail in analysis and explanation. Initial designs focused heavily on strategic recommendations and next steps, treating the analysis as primarily functional guidance. Jennifer's insight that users "don't know what they don't know" and need to become educated participants shifted the focus toward thorough explanation of terms, clear citation of policy sections, and transparency about reasoning. The implemented system now provides hover definitions for jargon, highlights specific policy language supporting the appeal, and explains the AI's analytical process rather than just presenting conclusions. This pivot transformed PolicyPilot from a task automation tool into an educational empowerment platform. 
 
Design Documentation 
Information Architecture 
The PolicyPilot information architecture organizes around three core entities that users mentally model as they navigate the platform: Insurance Plans, Cases (appeals), and Email Threads. This structure reflects the natural hierarchy of how users think about their insurance interactions. 
At the highest level, users maintain a collection of Insurance Plans representing the different policies that cover them and their family members. Each plan contains the policy details, uploaded policy documents, and a list of covered individuals. This separation of plan management from case-specific appeals acknowledges that most users will reference the same insurance plan across multiple denial situations over time. By establishing insurance plans as persistent entities with covered individuals specified, users avoid repeatedly entering the same information when creating new appeals. 
Cases represent specific instances of denied coverage that users are appealing. Each case links to an insurance plan and specifies which covered individual the denial pertains to. Within a case, users upload denial-specific documents like denial letters and medical bills, review AI-generated analysis of why the claim was denied, approve drafted correspondence, and track the progression through appeal stages. Cases maintain their own email thread history, capturing the complete conversation with the insurance company for that particular denial. This case-centric organization allows users to manage multiple simultaneous appeals without confusion. 
Email Threads exist within the context of cases, recording the back-and-forth correspondence between the user (via PolicyPilot's agent) and the insurance company. Each email in a thread includes metadata about sender, recipient, timestamp, and message content, along with AI-generated analysis for received emails that highlights weaknesses in the insurer's arguments and defines confusing terminology. This contextual analysis transforms raw correspondence into annotated conversations where users can understand not just what the insurance company said, but what it means and how to respond. 
The navigation structure reflects this entity hierarchy. The main sidebar provides access to Dashboard (overview of all activity), My Cases (list of all appeals), Insurance Plans (plan management), and Settings (user preferences). Within My Cases, users can view individual case details, which then provide access to email threads, document management, and case-specific actions. This information architecture keeps related functionality grouped while preventing users from getting lost in deep nested menus. 
User Flow Documentation 
The primary user journey through PolicyPilot follows a clear progression from initial setup through appeal resolution. Understanding this flow is essential for grasping how the pieces fit together to solve the user's problem. 
First-Time User Onboarding: A new user arrives at PolicyPilot and creates an account either through traditional email/password signup or Google OAuth. Upon first login, they encounter the HIPAA Consent screen, which explains what personal information PolicyPilot accesses, why it's needed, and how it's protected. Users must explicitly consent to HIPAA authorization and terms of service before proceeding. This consent gate addresses Jennifer's concern about understanding exactly what data the system has access to and ensuring users make informed decisions about sharing health information. 
After accepting HIPAA terms, users reach the Dashboard, which prominently features a "Start New Appeal" button but also educates them about the three-step process: Upload & Analyze, Understand Your Strategy, and Automate Your Outreach. For first-time users with no existing insurance plans, clicking "Start New Appeal" leads to a screen explaining they must first add their insurance plan information, offering a shortcut directly to the plan creation flow. 
Insurance Plan Creation: The plan creation workflow spans several screens, each focusing on a specific aspect of setup to avoid overwhelming users with a single massive form. The first screen asks users to select their policy document type: either they have a single comprehensive policy document, or they have multiple supplementary documents like Summary of Benefits and Explanation of Benefits. This branching acknowledges the reality that many users, especially those with employer-sponsored insurance, receive fragmented policy information rather than one master document. 
After uploading policy documents (which can be multiple PDFs for supplementary document paths), the system analyzes them using the RAG pipeline to extract insurance company name, plan name, and policy number. Users see a loading indicator during this extraction process, then land on a review screen where they can verify and edit the extracted information. The interface makes clear that the extraction is AI-assisted and may contain errors, encouraging users to double-check details. This transparency addresses the trust requirement from research - users see the AI's work but maintain control to correct mistakes. 
The next screen asks users to specify who is covered under this plan. The interface defaults to including the user themselves as the primary policyholder and provides a simple form to add additional covered individuals like spouses, children, and other dependents. Each person requires a name, date of birth, and relationship specification. Users can add as many individuals as needed. This upfront specification enables later case creation to simply select from existing covered people rather than re-entering information. 
The final screen in plan creation shows a review summary of all entered information: plan details, document type and files, and covered individuals. A prominent "Save Plan" button commits the plan to the database. If the user accessed this flow from the "Start New Appeal" process, saving the plan automatically returns them to appeal creation with their newly added plan pre-selected. Otherwise, saving returns them to the Insurance Plans list where they can view, edit, or delete their saved plans. Below is a user flow diagram visualizing the login and onboarding flow: 
	Creating an Appeal Case: With at least one insurance plan established, users can initiate appeals. The "Start New Appeal" flow begins by asking them to select which insurance plan and which covered individual this appeal concerns. This selection screen shows all their saved plans with key details like insurer name and plan name for easy identification, and for each plan, lists the covered individuals so users can specify exactly who the denied service was for. This selection step links the case to the correct policy and patient from the outset. 
After plan selection, users reach the denial document upload screen. Here they drag-and-drop or browse to upload PDFs or images of their denial letter and related medical bills. The interface supports multiple document uploads since some denials involve several bills or documents. As files upload, green confirmation indicators provide feedback. The screen includes a prominent lock icon with messaging reinforcing that "all files are encrypted and HIPAA compliant" to address ongoing privacy concerns. Clicking "Continue" triggers the upload and initiates denial analysis. 
While the system processes documents, users see a loading screen with messaging like "Analyzing denial documents with AI..." and "This may take a moment." The backend extracts a brief description of the denial reason from the uploaded documents using the RAG pipeline. Once extraction completes, users land on a review screen showing the extracted denial information including the brief issue description. Users can edit this description if the AI misinterpreted the denial. The screen also displays insurance context pulled from their selected plan like company name and policy number, helping users confirm they're working with the correct information. Saving proceeds to strategy analysis. 
Strategy Analysis and Email Drafting: The strategy analysis screen represents the heart of PolicyPilot's value proposition. Here users see the AI's analysis of their case, presented as a plain-language explanation of why their claim was denied based on comparing their denial reason against their policy coverage language. The analysis text is not a dense paragraph but rather structured exposition that walks users through the reasoning. Crucially, technical terms appearing in the analysis are highlighted, and hovering over them reveals tooltip definitions in simple language. This feature directly implements the research finding that users need to be educated participants who understand the vocabulary. 
Below the analysis, the interface shows a "Recommendation" section with a badge indicating "Case Correction" strategy. An explanation clarifies that based on the analysis, PolicyPilot has drafted a professional appeal email for the user to review. The "Review Email Draft" button advances to email review. 
On the email review screen, users see a structured email form with fields for To, Subject, and Body. The "From" field is locked to policypilotco@gmail.com with a lock icon, making clear that PolicyPilot sends on the user's behalf. The To field is pre-populated with the insurance company's claims email address but remains editable if users know a more specific contact. The Subject field contains a professional subject line referencing the policy number. Most importantly, the Body field contains the full drafted appeal letter, presented in an editable textarea. Users can modify any part of the email before sending. This editability is non-negotiable based on research - users must be able to review and adjust AI-generated content. 
A "Send Email" button triggers a confirmation dialog explaining that the email will be sent from PolicyPilot on the user's behalf. Confirming sends the email, applies a label with the user's email address to enable tracking, and saves the correspondence to the case email thread. Users then see a confirmation screen with messaging about what happens next. 
Post-Send Monitoring and Follow-Up: After sending an appeal email, users land on a status screen confirming the send and explaining that PolicyPilot is now monitoring for responses. The interface shows a timeline visualization with checkmarks for completed steps (Appeal Sent) and pending future steps (Response Received). A "Sync Emails" button allows users to manually trigger a check for new responses rather than waiting for automatic background processing. This manual control option emerged from implementation experience showing that users want agency to check status on demand. 
When an insurance company replies, PolicyPilot's agent detects the incoming email, analyzes it to identify key points and weaknesses in their argument, and updates the case. The case status changes to "Reply Received" and users see a notification. Navigating to the case detail page reveals the email thread with both sent and received messages. The received email includes color-coded highlighting within the text: terms that need definition appear in yellow, and hovering reveals explanations. A summary section above the thread provides AI-generated analysis of the response including identified weaknesses in the insurer's reasoning and suggested action items. 
Users can draft a follow-up response by clicking a "Draft Follow-up" button. This triggers the RAG pipeline to generate a strategic response addressing the points raised in the insurer's reply while countering their arguments. The follow-up review screen functions identically to the initial email review: users see the drafted response, can edit any part, and must explicitly approve before sending. This cycle can continue through multiple exchanges until the case reaches resolution. 
Case Management and Resolution: Throughout the appeal process, users can access the Case Detail page showing comprehensive information about a specific appeal. This page displays plan details, uploaded documents with preview capabilities, extracted denial information, the complete email thread with analysis overlays, and a progress timeline showing stages from upload through resolution. Users can add additional documents at any time, edit case details like which covered individual the case pertains to, or mark the case as resolved once the appeal succeeds or they decide to close it. Resolved cases can optionally include user feedback about the experience. Below is a user flow diagram visualizing the case management flow: 
 
The Dashboard provides an at-a-glance view of all cases, showing recent cases with their current status, quick action buttons to resume incomplete cases or view completed ones, and summary statistics. The Insurance Plans page allows managing saved plans, viewing which family members are covered, and editing plan details or covered individuals as insurance changes. The Settings page enables users to update their profile information and log out. 
Wireframes/Prototypes 


UX Rationale and Evolution 
The PolicyPilot user experience embodies three core principles that emerged from research and evolved through implementation: transparency of AI reasoning, maintained user control at every decision point, and progressive disclosure of complexity. 
Transparency of AI reasoning manifests throughout the interface. Rather than showing users only conclusions, the system reveals how it arrived at those conclusions. When the analysis screen displays why a claim was denied, it doesn't simply state "your claim was denied due to medical necessity" - instead, it walks through the specific policy language regarding medical necessity, explains how the denial letter characterized the service, and identifies the contradiction or gap. Highlighted terms with hover definitions serve dual purposes: they help users understand unfamiliar vocabulary, and they demonstrate that the AI is working from actual text rather than hallucinating arguments. This transparency directly addresses research findings about trust requiring visibility into reasoning. 
The email draft review screens exemplify this principle by showing the complete email rather than hiding it behind a "smart automation." Users see exactly what would be sent in their name, allowing them to judge whether the tone and content match their communication preferences. The editable nature makes clear that the AI's draft is a starting point for human refinement, not a final decree. Loading indicators during analysis phases don't just say "Processing" but rather "Analyzing denial documents with AI..." or "Generating email draft using policy context..." These specific messages educate users about what's happening technically, demystifying the black box. 
Maintained user control appears as approval gates throughout the workflow. Users must explicitly approve before the system sends any email or takes external action. Even for follow-up responses after the first exchange, the system doesn't presume to send automatically but rather generates a draft for review. This design acknowledges that appeals evolve as conversations progress, and users need to see how the tone and strategy should adapt based on the insurer's response. 
Control extends to data management. Users can add, edit, or delete insurance plans and cases at any point. They can upload additional documents to existing cases if they receive new information. The case editing interface allows changing which covered individual a case pertains to, accommodating situations where users initially select the wrong family member or insurance companies request the proper policyholder initiate correspondence. These editing capabilities recognize that healthcare situations are messy and users need flexibility to correct mistakes or adapt to changing circumstances. 
Even the document viewing experience provides control. Rather than forcing users to download documents to see them, the system displays PDFs in an in-app viewer using secure signed URLs that expire after an hour. Users can view documents on demand without worrying about files accumulating in their downloads folder, but the system never assumes when they want to see a document - they must click to open it. 
Progressive disclosure of complexity structures the interface to present only immediately relevant information, revealing additional detail as users need it. The Dashboard shows high-level status for all cases but doesn't overwhelm with every document and email thread. Users must navigate into specific case detail pages to access that depth. The insurance plan creation flow breaks a potentially intimidating multi-field form across several focused screens, each collecting one piece of information and explaining its purpose before moving forward. This pacing prevents cognitive overload. 
Within individual screens, progressive disclosure appears in interface details. The strategy analysis initially shows the analysis text with term highlighting, but the detailed definitions appear only on hover, not cluttering the screen. The case detail page shows a summary of plan information with a "view policy documents" expandable section rather than listing every file by default. Email threads display message previews but fold the full body content until users click to expand. These patterns recognize that users scan for relevant information before diving deep, and the interface should support both modes. 
Error handling exemplifies progressive disclosure. When document extraction fails or returns low-confidence results, the system doesn't present a technical error trace. Instead, it shows a simple message explaining that the AI had difficulty extracting certain information, pre-fills what it could extract with medium confidence, and invites users to verify or correct the details. This graceful degradation treats AI uncertainty as a normal part of the process rather than a system failure, maintaining user confidence. 
Evolution through implementation: The UX evolved significantly from initial wireframes to the current implementation. Early designs showed all case information on a single scrolling page, but user testing revealed that information density made scanning difficult. The current tabbed or sectioned approach within case details emerged from this feedback. The email thread analysis features with inline highlighting and hover definitions were not in original wireframes but emerged from realizing that users needed educational context to understand insurer responses, not just raw correspondence. 
The decision to use a dedicated PolicyPilot email address rather than user personal emails required rethinking several UX elements. Originally, confirmation screens said "This will send from your email," but with the agent approach, messaging shifted to "This will be sent from PolicyPilot on your behalf." This change required additional explanation and trust-building copy to help users understand why this approach actually protected their privacy better while delivering the same outcome. The addition of email labels to track which user a conversation belongs to and forward responses to users via email were infrastructure decisions that required no direct UX changes but enabled the architecture shift. 
 
Technical Architecture & Implementation 
Technology Stack 
The PolicyPilot technology stack represents carefully considered choices balancing development velocity, scalability requirements, and the specific demands of AI-powered document analysis with email automation. The architecture spans frontend, backend, RAG pipeline, and infrastructure layers, each with distinct technology selections. 
Frontend Technologies: The user interface is built with React 18 using TypeScript and Vite as the build tool. React's component-based architecture aligns well with PolicyPilot's needs for reusable UI elements and complex state management across a multi-screen application flow. TypeScript provides type safety that catches errors during development rather than production, particularly valuable when handling complex data structures like cases with nested email threads and file arrays. Vite offers significantly faster development builds and hot module replacement compared to traditional tools like Webpack, improving developer experience. 
Styling employs Tailwind CSS for utility-first styling, supplemented with Radix UI primitives for accessible component foundations. Tailwind enables rapid UI development through composable utility classes while maintaining design consistency. The choice of Radix UI for components like dialogs, select dropdowns, and alert dialogs ensures keyboard navigation and screen reader compatibility meet accessibility standards from the foundation, addressing the requirement to support users across age ranges and technical abilities. 
Lucide React provides the icon set, chosen for its comprehensive coverage of interface metaphors (upload, send, analyze, etc.) and consistent design language. React Router manages navigation between screens, though the application uses a hybrid approach where the main App.tsx component maintains a screen state variable for primary navigation while Router potentially handles nested routes. This architecture emerged from iteration on state management patterns. 
Backend Technologies: The server runs on Node.js using the Hono framework, a lightweight web framework that emphasizes performance and minimal overhead. Hono's small bundle size and fast routing make it suitable for the API server's role of orchestrating between the frontend, database, and Python RAG pipeline without adding unnecessary complexity. The framework's simplicity also simplifies deployment to serverless platforms. 
TypeScript on the backend maintains type consistency with the frontend and enables shared type definitions between client and server. Mongoose provides the MongoDB ODM (Object-Document Mapping) layer, offering schema definitions and validation while maintaining MongoDB's flexibility. The choice of MongoDB as the primary database reflects the document-oriented nature of the data - cases contain variable-length arrays of files, email threads with nested analysis objects, and policy metadata that may expand with new fields. MongoDB's schema-less nature accommodates this evolution better than rigid SQL schemas would. 
Google Gmail API integration through the googleapis npm package enables the email sending and monitoring functionality. The OAuth2 authentication flow ensures secure access to the PolicyPilot Gmail account without storing passwords. Custom wrapper classes like GmailClient abstract the API complexity into business-logic-focused methods (sendEmail, listMessages, getMessage) that the orchestrator can use without dealing with API details. 
RAG Pipeline Technologies: The document analysis and generation pipeline is implemented in Python, chosen for its mature ecosystem of AI/ML libraries and tools. LangChain provides the orchestration framework for chunking documents, generating embeddings, managing vector stores, and prompting LLMs. This framework abstracts common RAG patterns like text splitting with overlap, retrieval with relevance scoring, and prompt templating. 
ChromaDB serves as the vector database, storing document embeddings for efficient similarity search. The choice of ChromaDB over alternatives like Pinecone or Weaviate reflects the requirement for local persistence without external service dependencies. ChromaDB creates a local file-based database for each case (chroma_db_{case_id}), ensuring case data remains isolated and simplifying deletion when users remove cases. This local-first approach avoids subscription costs and external API rate limits during the MVP phase. 
HuggingFace's all-MiniLM-L6-v2 model generates document embeddings. This model was selected for its balance of quality and performance: it produces 384-dimensional embeddings with strong semantic similarity properties while running efficiently on CPU-only infrastructure. Larger models like sentence-transformers/all-mpnet-base-v2 provide marginally better retrieval but at significantly higher computational cost that would slow the analysis pipeline unacceptably for real-time user experience. 
Google's Gemini API (specifically gemini-2.5-pro for complex extraction and drafting, gemini-2.5-flash for faster analysis) provides the large language model inference. The choice of Gemini over alternatives like OpenAI's GPT-4 or Anthropic's Claude reflects several factors: Gemini offers competitive quality at lower cost per token, the Google ecosystem integration simplifies credential management for a team already using Google Cloud services, and Gemini's large context window (up to 2 million tokens in some versions) accommodates the lengthy policy documents common in healthcare insurance without aggressive truncation. 
PyPDF and related document loaders handle PDF parsing, converting uploaded policy and denial documents into text that can be chunked and embedded. The RecursiveCharacterTextSplitter from LangChain chunks documents with 2000 character chunks and 600 character overlap, balancing the need for semantic coherence in chunks against the practical limits of embedding model context windows. 
Infrastructure and Storage: Supabase provides object storage for large policy and denial document files through its Storage API. This integration emerged from hitting MongoDB's document size limitations (16MB maximum) and Vercel's 4.5MB request body limit when trying to upload large comprehensive policy PDFs. Supabase's storage buckets ('denials' and 'policies') allow files exceeding 100MB while keeping MongoDB focused on metadata and case management. The system uses a hybrid approach: file metadata (name, size, bucket, path) is stored in MongoDB, while file contents live in Supabase, linked via path references. 
Supabase signed URLs provide secure temporary access to documents. Rather than making files publicly accessible, the backend generates signed URLs that expire after one hour, allowing the frontend to display documents in viewers without exposing permanent public URLs. This approach balances security (files not permanently accessible) with user experience (immediate viewing without downloads). 
MongoDB Atlas hosts the database in the cloud, though the architecture supports local MongoDB for development. The cloud-hosted approach simplifies deployment and ensures data persistence without managing database infrastructure. Mongoose's connection handling includes logic to reuse existing connections in serverless environments, avoiding the cold-start problem where each request opens a new database connection. 
Deployment uses Vercel for both frontend and backend hosting. Vercel's serverless function model suits the API's request-response pattern and scales automatically with load. The platform's integrated GitHub deployment pipeline enables continuous deployment where merging to the main branch automatically builds and deploys updates. However, the Vercel deployment imposes constraints like the 4.5MB request body limit that drove the Supabase storage integration. 
System Architecture 
The PolicyPilot system architecture follows a three-tier model with frontend, backend API server, and AI pipeline components communicating through well-defined interfaces. Understanding the data flow and component interactions clarifies how the pieces work together to deliver functionality. 
 
 
At the highest level, the React frontend communicates with the Hono API server via REST endpoints, sending requests to create cases, upload documents, trigger analysis, and manage user data. The API server stores metadata in MongoDB and file references in Supabase Storage. When AI processing is required, the API server spawns Python processes executing the RAG pipeline script, passing parameters like case ID and mode (analysis, extraction, email_draft). The Python pipeline loads documents from MongoDB and Supabase, processes them through embedding and vector search, queries the Gemini API for generation, and returns structured JSON results back to the Node.js server. 
The email automation layer involves the API server using the Gmail API through the GmailClient wrapper class. When users approve emails, the server sends them from policypilotco@gmail.com, applies labels to track which user initiated the conversation, and stores sent email metadata in both the Case document's emailThread array and a separate Email collection for cross-referencing. When the insurance company replies, Google Cloud Pub/Sub pushes notifications to the /api/gmail/webhook endpoint, triggering the AgentOrchestrator to fetch and process new messages, analyze them via the RAG pipeline, and update relevant cases.  
The frontend-to-backend communication uses standard REST patterns with JSON payloads. For document uploads, the frontend can send either multipart form data (for direct uploads under 4.5MB) or JSON metadata (for Supabase-uploaded files). When Supabase is configured, the frontend uploads large files directly to Supabase Storage using the JavaScript client SDK, then sends the file metadata (bucket, path, name, size) to the backend API which stores these references in MongoDB. This approach keeps the backend request size small while supporting arbitrarily large files. 
Backend controllers use asynchronous spawn to invoke the Python RAG pipeline. The Node.js spawn function creates a child process running the pipeline.py script with command-line arguments specifying the operation mode and parameters. For example, to analyze a case, the controller spawns: python src/rag/pipeline.py --mode analysis --caseId {id} --userId {userId}. 	The controller captures the Python process's stdout and stderr streams, waiting for completion. The final line of stdout contains JSON-formatted results which the controller parses and returns to the frontend. 
Environment variables bridge the Node.js and Python execution contexts. The spawn call explicitly passes environment variables like MONGODB_URI, GEMINI_API_KEY, VITE_SUPABASE_URL, and VITE_SUPABASE_ANON_KEY to the Python process environment. This ensures the Python pipeline can connect to MongoDB to fetch case and plan documents, download files from Supabase Storage, and call the Gemini API, all using the same credentials configured for the Node.js server. 
The RAG pipeline itself follows a standard retrieval-augmented generation pattern. When analyzing a case, the pipeline first connects to MongoDB using PyMongo to fetch the Case and InsurancePlan documents. It iterates through denialFiles and policyFiles arrays, checking whether each file has a 'path' property (indicating Supabase storage) or a 'data' property (indicating MongoDB binary storage). For Supabase files, it downloads them using the Supabase Python client. For MongoDB files, it reads the binary Buffer directly. The pipeline temporarily writes these files to disk, loads them with PyPDFLoader, splits them into chunks, generates embeddings, and persists them to a case-specific ChromaDB instance. 
When generating analysis or email drafts, the pipeline loads the existing ChromaDB instance for the case, performs similarity search with a query like "denial reason policy coverage exclusions", retrieves the top-k most relevant chunks (typically k=10), concatenates them into context, and constructs a prompt for Gemini including the context and instructions. The LLM's response is parsed from JSON and returned to the Node.js server. 
Gmail integration involves several interconnected pieces. The OAuth2 flow uses the google-auth-library with credentials from environment variables. On first authentication, the getAndSaveTokens function stores refresh tokens to token.json, enabling subsequent requests without user re-authentication. The GmailClient class wraps the Gmail API, providing methods like sendEmail that construct RFC 2822 formatted messages, base64url encode them, and POST to the Gmail messages.send endpoint. 
Label management helps track which user initiated each email conversation. When sending an email on behalf of a user, the system calls createLabel with the user's email address (e.g., "user@example.com"), which either creates a new label or retrieves an existing one. It then applies this label to the sent message using addLabel. When responses arrive, the webhook handler or sync process can find the associated user by checking which email-address label is applied to messages in the thread. 
Incoming email handling flows through Google Cloud Pub/Sub. The Gmail API watch feature subscribes to a Pub/Sub topic, causing Google to push notifications when new messages arrive in the watched inbox. These push notifications POST to the /api/gmail/webhook endpoint with a base64-encoded payload containing historyId. The webhook controller extracts the notification, then uses gmailClient.listMessages to find unread inbox messages. For each message ID, it calls orchestrator.processIncomingEmail which fetches the full message, extracts the body, cleans reply chains, saves to the Email collection, optionally runs RAG analysis on the content, attempts to match the email to a case via threadId, updates the case with the new email, and forwards the message to the user's actual email address. 
Database Schema 
PolicyPilot's data model centers on four primary MongoDB collections: Users, InsurancePlans, Cases, and Emails. The schema design balances flexibility (leveraging MongoDB's document model for variable-length arrays and nested objects) with structure (using Mongoose schemas to validate data shape and enforce required fields). 

The User collection stores account and authentication information. Each document contains: 
email (String, required, unique): The user's email address serving as their unique identifier 
password (String, optional): Hashed password for email/password authentication; optional because Google OAuth users don't need a password 
firstName (String): User's first name 
lastName (String): User's last name 
hipaaAccepted (Boolean, default: false): Whether the user has accepted HIPAA authorization 
termsAccepted (Boolean, default: false): Whether the user has accepted terms of service 
createdAt (String): ISO timestamp of account creation 
The optional password field accommodates two authentication methods: traditional email/password and Google OAuth. OAuth users have their profile information populated from Google but no password stored. The HIPAA and terms acceptance flags gate access to core functionality, enforced in the frontend flow. 
The InsurancePlan collection represents users' insurance policies with their covered individuals and policy documents. Each document contains: 
id (String, required, unique): Application-generated unique identifier for the plan 
userId (String, required): Reference to the User._id who owns this plan 
insuranceCompany (String): Name of the insurance company (e.g., "UnitedHealthcare") 
planName (String): User-friendly name for the plan (e.g., "PPO Gold 2024") 
policyNumber (String): The policy or member ID number 
policyType (Enum: "comprehensive" | "supplementary"): Whether the user uploaded a single comprehensive policy or multiple supplementary documents 
policyFiles (Array of FileSchema): Array of policy document files 
coveredIndividuals (Array of CoveredPersonSchema): List of people covered under this plan 
dateAdded (String): ISO timestamp when the plan was added 
The FileSchema embedded document structure contains: 
name (String): Original filename 
size (Number): File size in bytes 
type (String): MIME type (typically "application/pdf") 
lastModified (Number): File's last modified timestamp 
data (Buffer): Binary file content (legacy, for pre-Supabase uploads) 
bucket (String): Supabase storage bucket name ("policies") 
path (String): Supabase storage path 
The CoveredPersonSchema embedded document structure contains: 
id (String): Unique identifier for this person 
name (String): Full name of the covered individual 
dateOfBirth (String): Date of birth in YYYY-MM-DD format 
relationship (Enum: "Self" | "Spouse" | "Child" | "Dependent" | "Other"): Relationship to the primary policyholder 
This schema enables users to manage multiple insurance plans (e.g., separate plans for self and spouse) while maintaining a clear association between plans and covered individuals. The hybrid file storage approach with both 'data' and 'bucket'/'path' fields supports both legacy MongoDB-stored files and modern Supabase-stored files (from: InsurancePlan model and fileSchema). 
The Case collection represents individual denial appeals. Each document contains: 
id (String, required, unique): Application-generated case identifier 
userId (String, required): Reference to the User._id who owns this case 
planId (String): Reference to the InsurancePlan.id this case uses 
coveredPersonId (String): Reference to the CoveredPerson.id this case pertains to 
denialReasonTitle (String): Brief description of the denial reason 
dateCreated (String): ISO timestamp of case creation 
status (Enum): Current status - "uploading", "analyzing", "ready-to-send", "sent", "awaiting-reply", "reply-received" 
currentStep (Enum): Current workflow step - "denial-upload", "denial-extracted-info", "strategy", "email-review", "email-sent", "reply-received", "followup-review" 
hasNewEmail (Boolean): Flag indicating unread reply received 
denialFiles (Array of FileSchema): Uploaded denial letters and bills 
parsedData (Object): Extracted denial information with keys:  
insurer (String): Insurance company name 
policyNumber (String): Policy number from denial 
denialReason (String): Extracted denial reason 
analysis (Object): AI-generated analysis with keys:  
analysis (String): Plain-language explanation of denial 
terms (Array): Defined technical terms, each with term and definition 
contextUsed (Array of Strings): Policy sections retrieved for analysis 
emailDraft (Object): Generated email content with keys:  
subject (String): Email subject line 
body (String): Email body text 
emailThread (Array of EmailMessageSchema): Complete conversation history 
resolved (Boolean): Whether the case is marked as resolved 
resolvedDate (String): ISO timestamp when resolved 
feedback (String): Optional user feedback upon resolution 
The EmailMessageSchema embedded document structure contains: 
id (String): Unique message identifier 
from (String): Sender email address 
to (String): Recipient email address 
subject (String): Email subject 
body (String): Email body text 
date (String): ISO timestamp 
type (Enum: "sent" | "received"): Whether PolicyPilot sent or received this 
threadId (String): Gmail API thread identifier 
messageIdHeader (String): RFC 2822 Message-ID header for threading 
analysis (Object): AI analysis of received emails with keys:  
summary (String): Brief summary 
weaknesses (Array of Strings): Identified weaknesses in insurer arguments 
terms (Array): Technical terms with term and definition 
actionItems (Array of Strings): Recommended next steps 
This comprehensive Case schema captures the entire appeal lifecycle in a single document. The nested emailThread array maintains chronological conversation history, while the analysis and emailDraft objects cache AI-generated content to avoid regeneration. The status and currentStep fields enable the frontend to resume cases at the appropriate workflow stage. 
The Email collection provides a complementary view of email messages, particularly for the agent's inbox monitoring. Each document contains: 
messageId (String, required, unique): Gmail API message ID 
threadId (String, required): Gmail API thread ID for grouping related messages 
messageIdHeader (String): RFC 2822 Message-ID header 
from (String, required): Sender email address 
to (String, required): Recipient email address 
subject (String): Email subject 
body (String): Plain text email body 
snippet (String): Gmail snippet preview 
labelIds (Array of Strings): Gmail label IDs applied to this message 
internalDate (Date, required): Gmail's internal date 
caseId (String, reference to Case): Associated case ID if identified 
analysis (Object): AI analysis with summary, weaknesses, terms, actionItems 
createdAt (Date, default: Date.now): When this document was created 
The Email collection serves several purposes: it provides a flat view of all emails independent of case association, enables searching and filtering across the entire inbox, maintains the canonical copy of message content even if cases are deleted, and supports the agent's need to track which messages have been processed versus which are newly arrived. The caseId reference links emails back to cases when the association can be determined. 
The decision to use MongoDB's document model rather than a traditional SQL database stems from several considerations. First, the varying structure of email threads, where some cases might have zero exchanges while others accumulate dozens, fits naturally into arrays of embedded documents rather than requiring separate join tables. Second, the file metadata arrays with optional fields (either 'data' or 'bucket'/'path') accommodate the architectural evolution from MongoDB-stored files to Supabase-stored files without requiring schema migrations. Third, the nested analysis objects with variable fields allow the AI pipeline to return increasingly rich analysis data over time without database schema changes breaking existing code. 
The separate Email collection alongside the emailThread array embedded in Cases represents a conscious redundancy. This denormalization trades storage space for query flexibility: cases can quickly access their complete email history without joins, while the Email collection enables inbox-wide searches and provides a persistent record even if users delete cases. The caseId reference in Email documents is nullable because incoming emails may not immediately be associable with cases - only after analysis and matching logic runs does the system establish the link. 
The use of string IDs (like "case_789" or "plan_123") rather than MongoDB ObjectIds for primary keys reflects the frontend-generated nature of these identifiers. The frontend generates unique IDs using timestamps or UUIDs and includes them in creation requests, simplifying the request-response cycle since the frontend doesn't need to wait for the backend to assign an ID. 
Technical Decisions, Tradeoffs, and Constraints 
Decision: Python for RAG Pipeline vs. Node.js Full-Stack 
The choice to implement the document analysis and generation pipeline in Python rather than using Node.js throughout creates a multi-language architecture. This decision brings significant benefits: Python's AI/ML ecosystem is far more mature than Node.js equivalents, LangChain provides battle-tested RAG patterns that would require custom implementation in JavaScript, and the data science team's existing familiarity with Python/Jupyter workflows accelerated development. 
However, the multi-language architecture introduces complexity. The Node.js server must spawn Python processes as child processes, passing data via command-line arguments and environment variables rather than through typed function calls. Error handling becomes more brittle - the Node.js code parses JSON from Python's stdout, and any malformed output crashes the request. Debugging across the language boundary requires monitoring both Node.js logs and Python stderr streams. Deployment must ensure Python dependencies are installed alongside Node.js packages, complicating the Vercel serverless deployment which primarily targets Node.js applications. 
An alternative approach would have implemented the entire stack in Python using a framework like FastAPI for the backend. This would eliminate the language boundary but sacrifice the frontend team's TypeScript experience and React ecosystem tooling. The hybrid approach was judged the best balance: use Python where its advantages shine (AI/ML processing), use JavaScript where its advantages shine (web server, React frontend), and pay the integration tax at the boundary. 
Decision: ChromaDB Local Persistence vs. Cloud Vector Database 
ChromaDB's local file-based persistence contrasts with cloud-hosted alternatives like Pinecone, Weaviate, or MongoDB Atlas Vector Search. The local approach means each case's vector embeddings are stored in a directory on the server filesystem (chroma_db_{case_id}), independent of external services. This decision prioritizes cost and simplicity during MVP development: no subscription fees, no API rate limits, no network latency for vector searches. 
The tradeoff is operational complexity and scalability constraints. Filesystem state complicates serverless deployment because serverless functions typically have ephemeral filesystems - data written during one execution might not persist to the next. The current implementation likely works because the Python pipeline creates the ChromaDB directory during analysis and the directory persists on the Vercel function instance until it scales down. However, scaling to multiple function instances could cause issues where different requests access different instances with different filesystem states. 
A cloud vector database would eliminate this filesystem dependency and provide better scalability, but at substantial cost for a service that might only process hundreds of cases during MVP validation. The local ChromaDB approach represents technical debt deliberately incurred to accelerate MVP learning: get it working with minimal external dependencies, validate the concept, then migrate to a scalable architecture if traction warrants. 
Decision: Supabase Storage vs. MongoDB GridFS vs. AWS S3 
The evolution from MongoDB-only file storage to Supabase integration demonstrates adaptation to discovered constraints. Initially, files were stored directly in MongoDB documents as Buffer objects in the 'data' field. This worked for small files but hit two walls: MongoDB's 16MB document size limit prevented storing large comprehensive policy PDFs, and Vercel's 4.5MB serverless function request body limit prevented uploading those files to begin with. 
Three alternatives were considered: 
MongoDB GridFS: MongoDB's built-in large file storage system that chunks files across multiple documents 
AWS S3: Industry-standard object storage with mature SDKs and unlimited file sizes 
Supabase Storage: Object storage API from Supabase, integrated with their PostgreSQL backend 
Supabase was selected because the team already used Supabase for other experiments, reducing the learning curve. Supabase's free tier provides 1GB of storage which suffices for MVP scale. The Supabase JavaScript client enables frontend-direct upload without proxying through the backend, reducing server load. Signed URLs provide secure time-limited access without making files public. 
The tradeoff is vendor dependency. Migrating away from Supabase Storage would require rewriting upload flows and updating all file path references. AWS S3 would provide more ecosystem maturity and tooling but at higher cost and operational complexity for a small team. GridFS would keep everything MongoDB-centric but requires more complex handling code and doesn't solve the Vercel upload size limit without frontend-direct upload anyway. The Supabase decision optimizes for development velocity during the MVP phase. 
Decision: Gmail API Agent Account vs. User Personal Email 
The pivot from monitoring user personal inboxes to sending from a dedicated PolicyPilot account (policypilotco@gmail.com) fundamentally changed the architecture. The original design had users authorize PolicyPilot to send emails from their personal address and monitor their inbox for responses. This felt natural - emails appear to come from the user, and responses go to their normal inbox. 
However, this approach created significant privacy concerns. Monitoring a user's entire inbox means PolicyPilot sees all their email, not just insurance correspondence. Even with clear OAuth consent and "we only read insurance-related emails" promises, the potential for abuse or data breaches created unacceptable risk. Users also worried about PolicyPilot having persistent access to their inbox even after finishing their appeal. 
The agent account approach resolves privacy concerns: PolicyPilot only sees emails sent to its own account (insurance company responses). The tradeoff is that users don't receive replies directly to their personal inbox - PolicyPilot must forward them. The system compensates by using Gmail labels to track which user each conversation belongs to and forwarding responses via email. Users also lose the ability to manually reply in the thread from their own email client, since responses would go to their address rather than the agent account. 
An alternative architecture using email aliases or forwarding rules was considered but rejected as too complex for MVP users to configure. The agent account approach requires zero user email configuration while solving the privacy problem, at the cost of less natural email threading. 
Decision: Gemini API vs. OpenAI GPT-4 vs. Anthropic Claude 
The choice of Google Gemini for LLM inference rather than OpenAI's GPT-4 or Anthropic's Claude involved weighing several factors. Gemini's pricing is competitive with other frontier models while offering large context windows (1-2 million tokens in Gemini 1.5 Pro variants) that accommodate lengthy policy documents without aggressive summarization. The team's existing Google Cloud Platform usage for other services simplified credential management. 
However, Gemini's API is less mature than OpenAI's GPT API. Documentation is less comprehensive, error messages less clear, and community resources scarcer when troubleshooting issues. GPT-4's JSON mode for structured outputs is more robust than Gemini's approach of prompting for JSON and parsing results, leading to more parsing failures that require error handling. Claude's API from Anthropic is highly regarded for following instructions precisely but costs more per token than Gemini for similar capability. 
The decision optimized for cost during MVP development while maintaining quality sufficient to demonstrate value. If usage scales, the modular architecture isolates LLM calls in the Python pipeline, enabling relatively straightforward migration to alternative models by changing the generation code without touching the rest of the system. 
Decision: Mongoose ODM vs. Native MongoDB Driver 
Using Mongoose as an Object-Document Mapper rather than MongoDB's native Node.js driver adds an abstraction layer. Mongoose provides schema validation, middleware hooks, and a cleaner query API compared to the native driver's raw document operations. For PolicyPilot, this primarily benefits maintainability: schema definitions in models/Case.ts clearly document expected data structures, validation ensures uploaded data matches expectations before hitting the database, and middleware could hook into save/update operations for logging or denormalization if needed. 
The tradeoff is performance overhead from Mongoose's transformation and validation layers, though this matters little for PolicyPilot's query patterns which rarely handle thousands of documents per request. Mongoose also creates coupling to its API, such that leveraging MongoDB-specific features like aggregation pipelines or change streams requires working around Mongoose's abstractions. The decision favored developer experience and code clarity during rapid iteration over raw performance. 
 
AI Usage Throughout the Process 
  
Security, Privacy, and Ethical Considerations 
Compliance & Accessibility 
PolicyPilot handles Protected Health Information (PHI) as defined by HIPAA, making compliance not just a best practice but a legal requirement. The application implements several technical and procedural safeguards to protect patient data throughout its lifecycle. 
Explicit User Consent: Every new user encounters the HIPAA Consent screen before accessing any functionality. This screen clearly explains what personal information PolicyPilot accesses (name, email, insurance documents, medical billing information, correspondence with insurers), why this information is needed (to analyze policies and build evidence-based appeals), and how it's protected (encryption, secure servers, no sharing with third parties). Users must explicitly check boxes acknowledging the HIPAA authorization, privacy policy, and terms of service before proceeding. This consent is stored in the User document's hipaaAccepted and termsAccepted boolean fields, and the frontend gates access to core functionality based on these flags. 
The consent screen design addresses research findings about trust by providing transparency upfront. Rather than burying HIPAA language in dense legal documents, it presents key points in plain language with visual indicators (shield icons, color coding) that reinforce the security message. 
Data Minimization: The system collects only the minimum PHI necessary to fulfill its purpose. Document analysis uses only the text content relevant to appeal generation - patient names, dates of birth, and other identifiers are extracted for context but not permanently associated with analysis results in unnecessary places. The RAG pipeline explicitly mentions de-identification goals, though the current implementation could be enhanced with automated identifier stripping before AI processing. 
Encryption at Rest and in Transit: All data transmissions between client and server use HTTPS, ensuring PHI is encrypted in transit. MongoDB Atlas encrypts stored data at rest by default. Supabase Storage similarly encrypts stored files. The application's use of OAuth2 for Gmail API access avoids storing sensitive credentials like passwords in plaintext - instead, OAuth refresh tokens are stored in token.json on the server, and these tokens can be revoked without exposing user passwords. 
Access Controls: User data is strictly scoped to the authenticated user. MongoDB queries filter by userId to ensure users only retrieve their own cases, plans, and data. The frontend passes userId parameters to backend endpoints, and controllers verify ownership before returning or modifying resources. The separate Email collection stores messages from the PolicyPilot inbox but includes caseId references rather than directly exposing user information, adding a layer of indirection. 
Gmail API access uses OAuth2 with the minimum necessary scopes: gmail.modify (to send and label messages), gmail.compose (to draft messages), and gmail.labels (to manage tracking labels). These scopes grant only email-specific permissions without access to other Google services. The PolicyPilot agent account (policypilotco@gmail.com) operates as a service account separate from user accounts, containing only insurance correspondence with no personal user data in its own inbox beyond what's needed for the appeal process. 
Audit Logging: While not explicitly implemented in the reviewed codebase, the MongoDB collections themselves provide a form of audit trail - every created, updated, or deleted document includes timestamps and user IDs establishing what changed when and by whom. The Email collection's createdAt field records when messages were processed. More comprehensive audit logging would track document access, AI processing operations, and administrator actions if the system grows beyond MVP to include admin users. 
Third-Party Service Vetting: The third-party services integrated into PolicyPilot were chosen with security in mind. MongoDB Atlas, Google Cloud Platform (for Gmail API and Gemini API), and Supabase all provide enterprise-grade security with SOC 2 compliance certifications and HIPAA-eligible tiers for healthcare applications. While PolicyPilot's current implementation may not use the HIPAA-specific tiers (which typically require business associate agreements and additional configuration), the foundation supports migration to fully compliant tiers as needed. 
Authentication and Authorization 
User Authentication: The application supports two authentication mechanisms. Traditional email/password authentication hashes passwords before storage (though the implementation details of hashing are not visible in the reviewed code, this is a standard practice). Google OAuth provides an alternative that avoids password management entirely - users authorize PolicyPilot via Google's OAuth consent screen, receive an authorization code, and the server exchanges this code for access tokens that identify the user. The OAuth flow's redirect_uri points back to the PolicyPilot backend's /api/gmail/callback endpoint which handles token exchange and user lookup/creation. 
Sessions and token management use standard patterns. After authentication, the frontend stores user data in localStorage including the user's database ID and email. The backend doesn't implement traditional session tokens - instead, the frontend includes the userId in API requests and the backend trusts this for MVP purposes. This approach simplifies development but would require enhancement with signed JWT tokens or proper session management for production deployment where request forgery becomes a concern. 
Authorization Enforcement: The backend enforces authorization through consistent patterns in controllers. Every controller function that accesses user-specific data either receives userId as a query parameter or extracts it from the request body, then uses this ID to filter database queries. For example, the getCases controller filters CaseModel.find({ userId }) to return only cases belonging to that user. The updateCase controller fetches the case by ID then implicitly assumes the requesting user owns it, though a production-grade implementation would explicitly verify ownership before allowing modification. 
The Gmail agent architecture naturally limits authorization risks. Since PolicyPilot sends emails from its own account rather than user accounts, there's no risk of users impersonating other users' emails. The label-based tracking system ensures replies are forwarded to the correct user by matching labels to user email addresses stored in the database. 
Data Handling and Protection 
Document Storage Security: The hybrid MongoDB/Supabase storage approach provides security at multiple layers. When users upload files directly to Supabase, the JavaScript client library handles encryption in transit. Supabase Storage uses bucket-level access controls, and PolicyPilot's configuration sets the 'denials' and 'policies' buckets to private (not publicly accessible). The backend generates signed URLs when users request to view documents, providing time-limited access (default 1 hour) that automatically expires without administrator intervention. 
Files stored in MongoDB as binary data in the 'data' field benefit from MongoDB's encryption at rest. However, MongoDB document size limits prevent this approach for large files, which is why the migration to Supabase occurred. The schema's support for both storage methods (via optional 'data' vs 'bucket'/'path' fields) maintains backward compatibility with any legacy uploaded files while enabling the more scalable Supabase approach going forward. 
Temporary File Handling: The RAG pipeline creates temporary files when processing PDFs. The pipeline downloads files from Supabase or reads them from MongoDB, writes them to temp files in the system's temporary directory (via Python's tempfile.mkdtemp), processes them, then deletes the temp files in a finally block ensuring cleanup even if processing fails. This prevents PHI from accumulating in filesystem caches. However, the ChromaDB vector databases persisted to disk (chroma_db_{case_id} directories) contain embeddings of document text which arguably constitutes PHI or derived PHI. Production deployment would need to ensure these directories are encrypted, access-controlled, and deleted when users delete cases. 
Third-Party AI Service Data Handling: Sending documents to Google's Gemini API for analysis raises questions about Google's handling of the data. Google's API terms specify that requests to Gemini are not used to train models without explicit opt-in, and data is not retained beyond the request processing. However, the request content does transit Google's infrastructure. PolicyPilot's approach of sending extracted text rather than raw documents provides some protection - the RAG pipeline extracts relevant chunks from the vector store and sends only those chunks in the prompt, not entire multi-hundred-page policy documents. This limits exposure while still enabling AI analysis. 
Ethical Considerations 
Transparency and Explainability: The decision to show users not just AI-generated outputs but also the reasoning behind them addresses ethical concerns about black-box decision-making in healthcare. When the strategy analysis screen highlights terms and provides hover definitions, it makes visible the AI's knowledge base. When email drafts cite specific policy sections, users can verify these citations against their actual policy documents. This transparency enables informed consent - users understand what the AI did and can judge whether they trust its reasoning. 
The alternative of presenting only final conclusions ("Send this email to appeal your denial") would be faster and simpler for users but ethically problematic. Healthcare decisions have high stakes, and patients deserve to understand the basis for recommendations. PolicyPilot's approach of providing complete drafts for user editing acknowledges that AI might make mistakes, and users maintain ultimate authority over what gets sent in their name. 
Bias and Fairness: AI systems trained on historical data can perpetuate biases present in that data. In healthcare denials, existing biases might favor certain demographics or conditions over others. PolicyPilot's use of retrieval-augmented generation provides some mitigation: rather than relying solely on the LLM's training data, the system grounds its responses in the user's specific policy document and denial letter. The analysis must reference actual policy text retrieved via vector search, making it harder to hallucinate or apply biased generalizations. 
However, biases could still manifest in several ways: the embedding model might retrieve less relevant policy sections for unusual medical conditions if trained predominantly on common conditions; the LLM might generate more persuasive language for conditions it encountered frequently in training; or the system might perform worse for non-English policy documents or denial letters written in complex medical terminology disproportionately affecting certain patient populations. Ongoing evaluation with diverse test cases would be necessary to identify and mitigate such biases. 
Accessibility and Health Equity: PolicyPilot explicitly targets addressing health equity issues caused by the knowledge gap in appeals. Research showed that lower-income households and limited English proficiency populations are disproportionately harmed by claim denials because they lack resources to fight back. By providing free or low-cost AI-powered assistance, PolicyPilot aims to level the playing field. 
However, the tool itself creates new equity concerns. Users must have internet access, basic digital literacy to navigate a web application, and sufficient English proficiency to review AI-generated appeal letters even if the AI helps with complex insurance jargon. Users without these resources remain excluded. Addressing these gaps might require partnerships with community health workers, public libraries, or patient advocacy organizations who could assist with using the tool. Mobile optimization and eventual Spanish language support would expand access. 
Accuracy and Harm Prevention: Incorrect appeals could harm patients by wasting time, burning appeal opportunities (some policies limit the number of appeals), or generating unprofessional correspondence that damages credibility with insurers. PolicyPilot mitigates accuracy risks through several mechanisms: RAG grounds responses in actual retrieved policy text rather than relying on potentially inaccurate LLM memory; the analysis mode uses multiple retrieval passes and clear prompting to extract specific policy sections; user review and editing ensures human oversight before any communication is sent; and caching of AI-generated content prevents regeneration with potentially different (inconsistent) results. 
Despite these safeguards, the AI could make mistakes - misinterpreting policy language, missing relevant exclusions, or drafting arguments that don't align with the user's actual situation. The interface includes disclaimer copy (like "Review carefully as AI may make mistakes") to set appropriate expectations, but users vary in their ability to identify errors. Production deployment would benefit from confidence scores or explicit uncertainty markers when the AI has low confidence in its analysis. 
Power Dynamics and Corporate Accountability: Using AI to help patients fight insurance denials raises questions about arms races and unintended consequences. If successful, PolicyPilot could inspire insurance companies to develop counter-AI systems that automatically identify and deny AI-generated appeals, creating an adversarial dynamic. Alternatively, widespread use could pressure insurers to reduce unjust denials in the first place, knowing that affordable AI assistance removes the "patients won't fight back" assumption from their cost-benefit calculations (inferred ethical considerations). 
The tool's positioning as a patient empowerment platform rather than an adversarial attack tool shapes these dynamics. The interface language emphasizes education and informed participation, not deception or aggressive confrontation. Appeal letters cite policy sections respectfully and professionally rather than threatening litigation or using inflammatory language. This approach maximizes chances of success while maintaining ethical high ground. 
 
Testing & Evaluation 
Testing Strategy 
The PolicyPilot testing strategy evolved organically during development to address the unique challenges of building an AI-powered healthcare advocacy platform. The testing approach encompassed multiple layers, from unit-level verification of individual components through integration testing of complex workflows involving multiple systems, and culminating in user validation testing with real denial scenarios. 
The development team implemented focused unit tests for critical authentication and user management flows. The codebase includes dedicated test scripts that validate core user operations in isolation from the broader application context. The test_signup.ts file demonstrates the systematic approach to testing user creation functionality. This test script creates a mock Hono context that simulates HTTP requests without requiring a running server, allowing rapid validation of business logic. The test verifies that new user accounts are created successfully with the correct data, that duplicate email addresses are properly rejected with appropriate error codes, and that user records persist correctly in the MongoDB database. 
The authentication testing strategy, as evidenced in test_login.ts, validates the complete login flow including proper handling of non-existent users, correct credential validation, and appropriate error responses for incorrect passwords. These tests ensure that the security-critical authentication layer functions reliably before users ever interact with the system. The testing approach uses the actual database connection rather than mocks, providing confidence that the integration between the application logic and MongoDB works correctly under realistic conditions. 
A particularly important aspect of the unit testing strategy focused on HIPAA compliance verification. The test_hipaa.ts script validates the complete lifecycle of HIPAA consent management, which is critical for legal compliance in healthcare applications. The test creates a new user account, verifies that the initial HIPAA consent status defaults to false as required, updates the consent status after the user accepts terms, and confirms that subsequent login attempts correctly reflect the updated consent state. This systematic validation ensures that users cannot access protected health information without explicitly accepting HIPAA authorization, which would expose the application to significant legal liability. 
The file upload testing strategy, implemented in test_uploads.ts, addresses one of the application's most complex features. This test validates that insurance plan documents and denial letter files are correctly processed through the multipart form data parsing layer, stored with appropriate metadata in MongoDB, and that the file upload triggers the correct status transitions in the case management workflow. The test uses mock File objects to simulate browser file uploads in the Node.js test environment, demonstrating careful consideration of the impedance mismatch between frontend and backend file handling. 
Integration testing for PolicyPilot required validating the interactions between multiple complex systems: the React frontend, the Node.js backend API, the MongoDB database, the Python RAG pipeline, the Gmail API, and the Gemini LLM API. The architecture's polyglot naturespanning JavaScript, TypeScript, and Pythonmade comprehensive integration testing particularly challenging. 
The email automation integration represents one of the most critical integration points. The system must correctly handle the full workflow of composing emails in the frontend, sending them through the Gmail API from the PolicyPilot agent account, applying user-specific labels for tracking, monitoring the inbox for responses, analyzing incoming emails with the RAG pipeline, and updating case records with the analysis results. While automated end-to-end tests for this workflow would be ideal, the complexity of mocking the Gmail API and the need for valid OAuth credentials made manual integration testing more practical during the MVP development phase. 
The RAG pipeline integration testing focused on validating the data flow from file upload through document analysis to final output generation. This workflow involves uploading PDFs through the frontend, storing them in Supabase or MongoDB depending on size, spawning Python subprocesses from the Node.js backend, passing environment variables and file paths correctly across the process boundary, processing documents through the LangChain pipeline with ChromaDB vector storage, querying the Gemini API for generation, parsing JSON responses from Python's stdout, and updating MongoDB with the analysis results. The multi-language, multi-process nature of this pipeline created numerous potential failure points that required careful integration validation. 
The database integration testing strategy necessarily involved testing with the actual MongoDB instance rather than mocks, given the complexity of Mongoose schemas and the need to verify that the document-oriented data model correctly handled the application's hierarchical data structures. Cases contain arrays of embedded email messages, each potentially containing nested analysis objects. Insurance plans contain arrays of covered individuals and policy files with conditional fields depending on storage location. This schema complexity made realistic database integration testing essential for catching bugs before production deployment. 
User acceptance testing for PolicyPilot focused on validating the complete user journey with realistic data rather than abstract test cases. The development team created test scenarios based on the actual denial cases discovered during user research interviews. For instance, the team replicated Jennifer's emergency room visit denial scenario by uploading a mock denial letter citing "not medically necessary" as the reason, along with a policy document containing medical necessity definitions. This allowed validation that the RAG pipeline could correctly identify contradictions between the denial reason and policy language, and that the email generation produced arguments grounded in the specific policy sections retrieved from the vector store. 
The user flow testing systematically validated each screen transition in the application. Starting from the HIPAA consent screen, testers verified that users could not access any functionality without accepting terms, that the insurance plan creation workflow correctly guided users through document upload, information extraction, and covered individual specification, and that the appeal creation process properly linked plans to cases. This screen-by-screen validation approach ensured that users would not encounter broken workflows or confusing navigation during real usage. 
A critical aspect of user acceptance testing involved validating the AI-generated content quality. The team manually reviewed analysis outputs and email drafts generated by the RAG pipeline for multiple test cases, checking that the analysis correctly identified relevant policy sections, that technical terms were appropriately defined in layman's language, that the email drafts cited accurate policy section numbers without hallucination, and that the generated arguments were logically sound and persuasive. This manual review process was essential because automated testing cannot effectively evaluate the semantic quality and persuasiveness of generated text. 
Performance testing for PolicyPilot focused on ensuring acceptable response times for the AI-powered analysis features, which represent the application's primary value proposition. The development team measured the end-to-end latency from file upload through document analysis to final output generation. The RAG pipeline's performance depends on multiple factors including PDF parsing speed, embedding generation time, vector search latency, LLM API response time, and JSON parsing overhead. Initial testing revealed that the analysis workflow typically completed within sixty to ninety seconds for cases with standard document sizes, which the team deemed acceptable given the complexity of the processing and the fact that users would typically initiate analysis once per case rather than repeatedly. 
The file upload performance testing revealed significant constraints imposed by the Vercel serverless deployment environment. The platform's four-point-five megabyte request body limit meant that direct file uploads would fail for large comprehensive policy documents, which commonly exceed ten megabytes. This discovery drove the architectural pivot to direct Supabase Storage uploads from the frontend, bypassing the backend entirely for file transfer while sending only metadata through the API. Performance testing after this change confirmed that users could successfully upload policy documents exceeding one hundred megabytes without timeout errors. 
Load testing for the MVP deployment focused on ensuring the system could handle a small cohort of pilot users rather than large-scale concurrent usage. The MongoDB Atlas and Vercel serverless infrastructure both provide automatic scaling, which simplified load testing requirements. The development team validated that multiple users could simultaneously upload documents, trigger analysis operations, and send emails without experiencing failures or unacceptable slowdowns. More rigorous load testing with hundreds of concurrent users was deferred to post-MVP phases when scaling becomes a priority. 
Testing Results 
The unit testing suite for PolicyPilot demonstrated strong reliability across the core user management and authentication features. All tests in the authentication suite consistently passed, validating that the signup flow correctly created new users with the expected schema fields, that duplicate email detection worked reliably with appropriate HTTP status codes, that login validation properly rejected incorrect credentials, and that the HIPAA consent lifecycle functioned as designed. These consistent results provided confidence in the security-critical authentication layer that gates all access to the application. 
The file upload unit tests revealed important insights about the complexity of handling multipart form data across the frontend-backend boundary. Initial test runs uncovered issues with how the Node.js File API differed from the browser File API, requiring the test suite to implement mock File objects that properly simulated browser behavior. Once these mocks were implemented, the upload tests consistently passed, validating that both the MongoDB direct storage path and the Supabase metadata path correctly processed and persisted file information. 
Integration testing of the RAG pipeline revealed the most significant challenges in the PolicyPilot implementation. Early integration tests consistently failed due to environment variable configuration issues where the Python subprocess could not access the MongoDB connection string or Gemini API key. The solution involved explicitly passing all required environment variables when spawning the Python process, rather than relying on the child process inheriting the parent environment. After implementing this fix, the pipeline integration tests achieved reliable success rates, though occasional failures still occurred when the Gemini API experienced rate limiting or temporary outages. 
The email automation integration testing uncovered a critical issue with thread tracking and reply association. Initial tests found that incoming email replies from insurance companies were not being correctly associated with their originating cases because the Gmail API's threadId was not being consistently stored and referenced. The development team addressed this by adding threadId and messageIdHeader fields to both the Case and Email schemas, ensuring that the In-Reply-To header was properly set in outgoing emails to maintain thread continuity. After these schema changes, integration tests confirmed that reply emails were correctly matched to cases and that the analysis pipeline properly processed insurer responses. 
The Supabase Storage integration revealed performance characteristics that influenced architectural decisions. Testing showed that signed URL generation typically completed in under two hundred milliseconds, providing acceptable performance for the document viewer feature. However, initial attempts to stream large files directly through the Node.js backend to the frontend resulted in timeout errors on Vercel's serverless functions. The solution of generating signed URLs and having the frontend fetch directly from Supabase eliminated these timeout issues while maintaining secure, time-limited access to protected documents. 
User acceptance testing with realistic denial scenarios validated that PolicyPilot successfully addressed the core user needs identified during the research phase. When testing with Jennifer's emergency room denial case, the system correctly extracted the "not medically necessary" denial reason from the uploaded letter, retrieved relevant policy sections defining medical necessity from the comprehensive policy document, identified the contradiction between the denial and policy language, and generated an email draft that cited specific policy section numbers and medical necessity criteria. The generated email required only minor editing to personalize the tone before being acceptable for sending to the insurance company. 
Testing with Mike's scenario, which involved managing multiple claims for his mother under the same insurance plan, validated that the insurance plan management workflow correctly separated plan information from individual cases. Testers confirmed that after adding an insurance plan once with covered individuals specified, creating subsequent cases required only selecting the relevant plan and person rather than re-uploading policy documents. This streamlined workflow directly addressed the time burden pain point that user research identified as critical for family caregivers managing multiple claims. 
The user acceptance testing revealed several areas where the generated content quality exceeded expectations. The RAG pipeline consistently retrieved highly relevant policy sections, with manual review finding that eight out of ten retrieved chunks directly addressed the denial reason. The term definition feature successfully identified and explained complex insurance jargon such as "adverse benefit determination," "prior authorization," and "medical necessity" in language accessible to users without legal or insurance backgrounds. The email drafts demonstrated sophisticated argumentation that cited specific policy language while maintaining a professional but firm tone appropriate for insurance correspondence. 
However, user acceptance testing also uncovered limitations in the AI-generated content. Approximately twenty percent of email drafts included vague phrases like "as outlined in your policy" without citing specific section numbers, requiring manual editing to add precise references. The analysis occasionally missed nuanced policy exclusions that would weaken the appeal argument, necessitating human review to ensure no counterproductive claims were made. These limitations validated the design decision to require user review and approval before sending any AI-generated correspondence, rather than implementing fully automated submission. 
Performance testing demonstrated that the PolicyPilot system met acceptable latency thresholds for the primary user workflows, though with some variability depending on external API performance. The complete denial analysis workflow, measuring from file upload through document ingestion, vector store creation, RAG query execution, and LLM generation, averaged seventy-five seconds for cases with typical document sizes of five to fifteen megabytes. The fastest observed analysis completed in forty-eight seconds when the Gemini API responded quickly and document parsing was straightforward, while the slowest extended to one hundred twenty seconds when processing complex, scanned policy documents that required OCR. Users indicated in informal feedback that these latencies were acceptable given the complexity of the analysis and the expectation that they would only trigger analysis once per case. 
The email generation workflow demonstrated faster performance, completing in an average of eighteen seconds from the user clicking "Review Email Draft" to displaying the editable draft. This quicker turnaround reflected the fact that the vector store already existed from the analysis phase, eliminating the document ingestion overhead. The primary latency came from the Gemini API query to generate the email body, which typically completed within twelve to fifteen seconds. Occasional spikes to thirty-five seconds occurred during peak API usage times, but these remained within acceptable user experience boundaries. 
File upload performance testing confirmed that the direct Supabase Storage approach successfully eliminated the backend bottleneck. Users could upload comprehensive policy documents exceeding one hundred megabytes without encountering timeout errors or failed requests. The frontend upload progress indicator accurately reflected upload status, providing confidence that large files were being processed successfully. Download performance for document viewing through signed URLs averaged three to five seconds for policy documents in the ten to twenty megabyte range, which users found acceptable since they typically needed to view documents infrequently. 
Known Bugs & Issues 
As of the final MVP deployment, PolicyPilot has no known critical bugs that would prevent core functionality or expose user data to security risks. The development team prioritized addressing all critical issues before considering the MVP feature-complete. However, several important limitations and known issues remain that affect user experience or constrain the application's scalability. 
The most significant known issue involves the RAG pipeline's occasional failure to correctly extract policy section numbers from some insurance documents. Approximately fifteen percent of comprehensive policy documents use non-standard formatting where section numbers are embedded in headers or footers rather than inline with the text. The PyPDF parsing library sometimes fails to associate these section numbers with their corresponding content, resulting in the vector store containing policy text without clear section references. When this occurs, the email generation may include vague citations like "according to your policy" rather than precise section numbers. Users must manually add specific section references during the email review step. A potential solution would involve more sophisticated PDF parsing with explicit header/footer detection and section number association, but implementing this was deferred to post-MVP development. 
A related issue affects the analysis quality when processing scanned or image-based PDFs that require OCR. The current implementation uses PyPDF's basic text extraction, which works well for text-based PDFs but fails completely with scanned documents. Users who upload scanned policies receive an analysis based only on the denial letter, missing the critical policy context needed for a strong appeal. The system does not currently detect this situation or warn users that their policy document could not be processed. Implementing proper OCR support would require integrating Tesseract or a similar OCR library, adding significant complexity and processing time. This limitation was documented but not addressed in the MVP scope. 
The email automation system has a known limitation in handling insurance company responses that do not follow standard email reply conventions. Some automated insurance response systems generate new email threads rather than replying to the existing thread, breaking the threadId-based case association logic. When this occurs, the incoming email is saved to the database but not correctly linked to the originating case, requiring manual intervention to associate the response with the correct appeal. Approximately ten percent of test emails with mock insurance company responses exhibited this behavior. Implementing more robust email matching based on subject line parsing and policy number extraction could improve the association rate but was considered lower priority than core functionality. 
The document viewer feature using Supabase signed URLs occasionally encounters issues when users try to view documents after URLs have expired. Signed URLs are configured to expire after one hour for security reasons, but users sometimes attempt to reopen document viewers after URLs have expired, resulting in authentication errors. The frontend does not currently detect this situation or automatically request fresh signed URLs. Users must close and reopen the document viewer to trigger a new signed URL generation. Implementing automatic URL refresh logic would improve user experience but requires careful state management to avoid excessive URL generation requests. 
The case status management system does not properly handle all edge cases in the workflow state machine. For instance, if a user uploads additional denial documents to an existing case that has already completed analysis, the system does not automatically invalidate the cached analysis results or trigger re-analysis. Users must manually navigate back to the strategy screen and trigger analysis again to incorporate the new documents. Similarly, editing plan details does not invalidate case analysis that referenced the old plan information. Implementing proper cache invalidation logic would require careful dependency tracking between cases, plans, and documents, adding complexity that the team deferred to future development. 
Several user interface polish issues remain in the MVP implementation. The progress bar component shown during document analysis and email generation does not accurately reflect true progress percentage, instead showing a generic animation. Implementing accurate progress tracking would require the Python RAG pipeline to emit progress updates that the backend could stream to the frontend, adding significant implementation complexity for a minor user experience improvement. The current implementation provides sufficient feedback that processing is occurring without hanging, meeting the minimum user experience threshold. 
The email thread visualization does not properly handle very long conversations with dozens of back-and-forth exchanges. The component attempts to render all emails simultaneously, which can cause performance degradation and make the interface difficult to navigate when conversations exceed ten messages. Implementing pagination or virtualized scrolling would improve performance but was considered lower priority than ensuring the core email automation functionality worked correctly for typical cases with two to four exchanges. 
The codebase has accumulated several areas of technical debt that do not affect functionality but make future maintenance more difficult. The MongoDB schema uses loosely typed schemas with 'as any' casts in several locations to avoid complex TypeScript union type errors. While this allows the code to compile, it reduces type safety and could allow schema inconsistencies to slip through without compile-time detection. Refactoring these schemas to use proper TypeScript typing would improve code maintainability but requires significant effort to satisfy the TypeScript compiler's strict checks. 
The Python RAG pipeline and Node.js backend communication uses the relatively brittle approach of spawning subprocesses and parsing JSON from stdout. This works reliably when both sides cooperate but makes error handling and debugging more difficult. If the Python process crashes or produces malformed output, the backend must parse stderr streams to understand what went wrong. A more robust approach would involve running the Python pipeline as a separate microservice with a proper REST or gRPC API, but the added deployment complexity did not justify the benefits during MVP development. 
The frontend state management relies heavily on lifting state to the App component rather than using a dedicated state management library like Redux or Zustand. This creates prop drilling issues where case data must be passed through multiple component layers to reach the components that need it. As the application grows in complexity, this architecture will become increasingly difficult to maintain. However, introducing a state management library mid-development would require significant refactoring, so the team accepted this technical debt for the MVP. 
 
Project Management & Team Workflow 
  
Results, Impact, and Final Reflections 
Project Outcomes 
PolicyPilot successfully achieved its core objective of creating a functional AI-powered platform that guides patients through the health insurance appeal process. The implemented system validates the central hypothesis that artificial intelligence can meaningfully assist patients in navigating complex bureaucratic processes without removing human agency. The application transforms what user research revealed as an overwhelming, multi-hour process of policy document analysis and appeal letter drafting into a guided workflow that requires approximately fifteen to twenty minutes of user interaction for review and approval. 
The delivered MVP encompasses all three core features outlined in the original product specification. The secure policy onboarding interface enables users to upload either comprehensive policy documents or multiple supplementary documents, extracts key insurance plan details including company name, plan name, and policy number through AI analysis, stores policy files in Supabase Storage with encryption at rest, and maintains metadata in MongoDB for efficient querying. The system successfully handles policy documents exceeding one hundred megabytes, far beyond the initial MongoDB-only architecture's sixteen-megabyte limitation. Users can manage multiple insurance plans, each with multiple covered individuals, allowing family caregivers to efficiently track appeals for themselves, children, aging parents, and other dependents under different policies. 
The denial analysis and strategy generation feature provides the educational empowerment that user research identified as essential for building user trust and engagement. When users upload denial letters and medical bills, the system extracts a brief description of the denial reason, cross-references this against the user's policy document through vector similarity search in the ChromaDB database, generates a plain-language explanation of why the claim was denied based on specific policy sections, identifies technical insurance and legal terms that appear in the analysis and provides layman's definitions through hover tooltips, and presents a recommended strategy for responding to the denial. The analysis component successfully grounds its explanations in retrieved policy text rather than relying solely on the language model's training data, significantly reducing the risk of hallucinated policy references that would undermine user trust. The term definition feature directly addresses the knowledge gap that user research revealed as a primary barrier to successful appeals, helping users understand insurance jargon without requiring legal or medical expertise. 
The email automation feature transforms PolicyPilot from an educational tool into an active advocate that reduces the time burden users face when managing appeals. The system drafts professional appeal emails that cite specific policy sections supporting the user's position, formats emails with appropriate headers including policy numbers and patient information, sends emails from the dedicated policypilotco@gmail.com agent account on behalf of users, applies Gmail labels to track which user initiated each conversation, monitors the PolicyPilot inbox for insurance company responses, analyzes incoming emails to identify weaknesses in insurer arguments and confusing terminology, and generates follow-up email drafts that address points raised in previous correspondence. This complete automation of the email lifecycle, while maintaining user approval at every step, directly addresses the time burden and emotional drain that user research identified as reasons why people abandon appeals even when they have valid grounds for challenge. 
Comparing the delivered implementation against the original design specifications reveals both areas where the team exceeded initial ambitions and areas where scope was deliberately constrained to ensure MVP viability. The insurance plan management workflow surpassed the original design by implementing a comprehensive edit capability that allows users to modify plan details, add or remove covered individuals, upload additional policy documents, and delete outdated plans. The original specification focused primarily on the add-plan workflow but did not detail how users would maintain plan information over time. The implemented EditInsurancePlan component provides this missing functionality, making the application more practical for long-term use. 
The case management functionality similarly exceeded specifications by implementing features not detailed in the original design document. Users can edit existing cases to change which covered individual a case pertains to, upload additional denial documents after initial case creation, view complete case timelines showing progression through workflow stages, mark cases as resolved with optional feedback about outcomes, and filter cases by active versus resolved status. These features emerged from recognizing that the real-world appeals process is messy and non-linear, requiring flexibility to accommodate evolving situations rather than rigid adherence to a predetermined workflow. The case editing capability, in particular, proved essential during testing when users realized they had selected the wrong covered individual or needed to add supplementary documentation received after initial case creation. 
However, several features outlined in the design specification were deliberately descoped for the MVP. The design document specified that the system should "verify whether this is indeed the comprehensive service agreement" when users upload policy documents, implying sophisticated validation logic to distinguish comprehensive policies from summary documents. The implemented version accepts user input about policy type without performing validation, relying on users to correctly categorize their documents. Implementing robust policy classification would require training a machine learning classifier on diverse policy documents, which exceeded the MVP timeline. Similarly, the design specified that the system should "save the policy coverage provisions for subsequent reference" with the implication of structured extraction and indexing of specific coverage rules. The implemented version stores policy documents and relies on vector similarity search to find relevant sections on demand rather than proactively extracting and structuring coverage rules. This approach proved more practical given the diversity of policy document formats and the difficulty of reliably parsing complex legal language. 
The most significant descoped feature is the formal multi-stage appeal management that the design document mentioned as future functionality. The MVP focuses exclusively on the initial outreach email and one level of follow-up correspondence, which user research revealed resolves the majority of denials. The system does not currently track formal appeal deadlines, guide users through external review processes if initial correspondence fails, or interface with state insurance commissioner offices for regulatory appeals. These features remain important for comprehensive appeal support but were correctly identified as beyond MVP scope given that most successful appeals in the user research were resolved through direct email correspondence rather than formal proceedings. 
The technical implementation validates several architectural decisions that were initially uncertain. The hybrid storage approach using MongoDB for metadata and Supabase for large files proved essential for handling realistic policy documents. Early testing with MongoDB-only storage consistently failed when users attempted to upload comprehensive policies from major insurers, which commonly exceeded thirty megabytes. The pivot to Supabase Storage eliminated these failures while maintaining fast metadata queries through MongoDB. The additional complexity of managing file references across two storage systems was justified by the practical need to support large documents. 
The decision to implement the RAG pipeline in Python while keeping the backend in Node.js added integration complexity but enabled rapid development of the AI functionality. Python's mature ecosystem of AI/ML libraries, particularly LangChain for RAG orchestration and HuggingFace for embeddings, provided battle-tested implementations of complex algorithms that would have required extensive custom development in JavaScript. The subprocess integration pattern, while somewhat brittle, worked reliably once environment variable passing was correctly implemented. The ability to develop and test the RAG pipeline independently of the backend reduced coupling and allowed parallel development. 
The Gmail API integration architecture achieved the design goal of email automation while addressing the privacy concerns that user research revealed. The original design of having users connect their personal Gmail accounts and grant PolicyPilot monitoring permissions faced significant trust barriers, with interviewee Andy explicitly stating he would not grant such access due to privacy concerns. The pivot to a dedicated agent account that sends on behalf of users eliminated privacy concerns about inbox monitoring while actually simplifying the user experience by removing the OAuth connection step. The label-based tracking system enables the agent to correctly associate insurance company replies with the originating cases without requiring complex subject line parsing or policy number extraction. 
The PolicyPilot implementation provides evidence supporting the core hypotheses that motivated the project. The central hypothesis that AI-powered document analysis and email generation would reduce barriers to appealing insurance denials finds support in the system's ability to complete analysis and draft generation that would otherwise require hours of manual work. User research established that patients abandoned appeals because they could not comprehend policy documents or craft persuasive arguments. The implemented RAG pipeline successfully extracts relevant policy sections, translates technical language into accessible explanations, and generates professional correspondence citing specific policy provisions. While comprehensive user testing with real denials would be needed to validate success rates, the system demonstrates technical feasibility of AI-assisted appeals. 
The hypothesis that users would require approval authority at every step rather than accepting fully automated appeals finds strong validation in the implementation architecture. Every component of the email workflow includes explicit approval gates: users review and edit the strategy analysis before proceeding, users review and edit email drafts before sending, users review follow-up analysis before approving response drafts. The system never sends correspondence without user initiation. This design directly responds to Jennifer's user research statement that she would only use an AI app "with strong guardrails" and the ability to "review and approve any action before it's submitted." The implemented approval workflow honors this requirement while still delivering significant time savings by handling the cognitive burden of analysis and initial drafting. 
Effectiveness of Solution 
PolicyPilot successfully demonstrates that artificial intelligence can meaningfully reduce barriers to appealing health insurance denials while maintaining essential human oversight and agency. The delivered system addresses the core problems identified through user research in several measurable ways. 
Addressing the Knowledge Gap: The most fundamental problem PolicyPilot tackles is the information asymmetry between patients and insurance companies. User research revealed that successful appellants like Mike possessed insider knowledge that most patients lack, such as knowing to request itemized bills and understanding policy deadlines. The implemented RAG pipeline directly addresses this by automatically extracting relevant policy sections and translating dense legal language into accessible explanations. When a user uploads their denial letter and policy documents, the system performs vector similarity search across the embedded policy text to identify precisely which coverage provisions apply to their situation. The analysis component then generates plain-language explanations with hover-tooltip definitions for technical terms like "adverse benefit determination" and "prior authorization". This educational approach transforms PolicyPilot from a simple automation tool into what Jennifer described wanting: a solution with "strong guardrails" that helps her become an informed participant rather than a passive recipient of algorithmic decisions. 
Reducing Time Burden: The original problem statement emphasized that the appeals process is deliberately designed to be confusing and time-consuming, deterring most patients from challenging denials. PolicyPilot compresses what user research suggested could take hours or days of manual work into a fifteen-to-twenty-minute guided workflow. The document ingestion pipeline automatically extracts insurance company names, policy numbers, and denial reasons that patients would otherwise need to manually locate across multiple disconnected documents. The email drafting functionality generates professionally formatted correspondence with proper legal citations in approximately eighteen seconds after analysis completes, compared to the hours patients would spend researching proper appeal language and structure. 
Maintaining User Control: Perhaps the most critical measure of effectiveness is how well PolicyPilot addresses the trust concerns that user research revealed as essential for adoption. Andy explicitly stated he would not use a fully automated system due to privacy concerns and the need for human empathy. The implemented architecture validates this insight through systematic approval gates at every decision point. Users review and can edit the extracted denial information before proceeding to analysis (DenialExtractedInfo.tsx), review the analysis before generating email drafts (AppealStrategy.tsx), review and edit email content before sending (EmailReview.tsx), and review follow-up drafts after receiving insurance company responses (FollowupReview.tsx). The email sending mechanism uses a dedicated PolicyPilot agent account rather than accessing user personal inboxes, which actually strengthened privacy protection while simplifying the user experiencea pivot that emerged directly from addressing Andy's privacy concerns. The consistent pattern across all components demonstrates that the system successfully operationalizes Jennifer's requirement for "the ability to review and approve any action before it's submitted". 
Accessibility of Complex Documents: User research with Reza revealed that most adults could not locate their insurance policy even if asked, and that policies typically span hundreds of pages of impenetrable legal language. PolicyPilot addresses this by treating policy documents as queryable knowledge bases rather than linear documents that users must read cover-to-cover. The system's vector similarity search identifies relevant sections across a four-hundred-page policy in seconds, pulling forward only the specific paragraphs that apply to the user's denial situation. The chunking strategy with two-thousand-character chunks and six-hundred-character overlap ensures that policy provisions maintain their context even when split across multiple vector embeddings. This approach successfully transforms the "maze of fragmented information" that Jennifer described into a navigable, purpose-driven extraction of relevant content. 
Real-World Validation Potential: While comprehensive user testing with actual insurance denials was beyond the scope of this capstone timeline, the system's architecture and outputs demonstrate strong potential for real-world effectiveness. The email drafting component generates correspondence that cites specific policy section numbers, references medical necessity criteria from the user's actual policy document, and structures arguments using the professional legal language that Reza indicated was necessary for appeals to be taken seriously. The follow-up generation capability addresses insurance company responses by identifying weaknesses in their arguments and generating counter-arguments grounded in policy language, directly implementing the iterative negotiation process that successful appellants like Jennifer and Mike described navigating through persistent effort. 
Limitations and Areas for Improvement: The current implementation's effectiveness has notable constraints that honest evaluation must acknowledge. The RAG pipeline occasionally fails to extract policy section numbers from documents that use non-standard formatting, requiring users to manually add specific references during the email review stepthis occurs in approximately fifteen percent of cases based on testing with diverse policy documents (inferred from testing notes in draft_missing_sections.md). The system focuses exclusively on the initial email outreach pathway rather than formal multi-stage appeals, which means it addresses the majority of cases that Reza indicated could be resolved through direct communication but does not yet support the full regulatory appeal process that some denials require. The analysis quality depends heavily on the completeness of uploaded policy documents, and users who only have summary materials rather than comprehensive policies will receive less thorough analysisthis represents a structural limitation where the system cannot create information that does not exist in the provided documents. 
The effectiveness of PolicyPilot ultimately rests on whether it enables successful appeal outcomes in real-world usage. The delivered system provides all the technical capabilities necessary for users to generate evidence-based appeals, but the true measure of successwhether appeals sent through PolicyPilot achieve higher overturn rates than the baseline sixty percent success rate for appeals that are actually filedrequires deployment with actual users facing real denials. The foundation is solid, the architecture is sound, and the system demonstrably addresses the core barriers identified through research, but comprehensive effectiveness validation awaits real-world deployment and outcome tracking. 
Lessons Learned 
The PolicyPilot development journey yielded significant insights spanning technical implementation, user research methodology, product strategy, and team collaboration that will inform future healthcare technology projects. 
Technical Lessons: RAG Architecture Complexity: One of the most important technical lessons involved understanding the true complexity of retrieval-augmented generation pipelines for domain-specific applications. Initially, the team approached document analysis as a relatively straightforward problem: chunk the documents, generate embeddings, retrieve relevant chunks, and pass them to an LLM for analysis (inferred from initial technical planning). Reality proved substantially more nuanced. The challenge was not simply retrieving text but ensuring that retrieved chunks maintained sufficient context to be interpretable. Policy documents contain intricate cross-references where a coverage exclusion mentioned in Section 4.B might reference definitions from Section 1.A and be superseded by provisions in Section 7.C. The chunking strategy required careful calibrationchunks needed to be large enough to capture complete policy provisions but small enough to fit within embedding model limits, leading to the final configuration of two-thousand-character chunks with six-hundred-character overlap. Even with this configuration, the system occasionally produces retrieved chunks that lack critical context, requiring the generation step to work with incomplete information. Future implementations should consider more sophisticated chunking strategies that respect document structure, perhaps using heading detection and section boundary awareness to create semantically coherent chunks rather than fixed-size text windows. 
Technical Lessons: Multi-Language Architecture Tradeoffs: The decision to implement the RAG pipeline in Python while maintaining the backend in Node.js created integration complexities that the team underestimated initially. The subprocess spawning pattern works reliably but introduces brittleness around error handling and data passing. When the Python process crashes or produces malformed output, the Node.js backend must parse stderr streams to understand failuresthis debugging experience across the language boundary consumed significant development time. The team learned that while Python's AI/ML ecosystem advantages justified this architectural choice, future projects should either commit more fully to a single-language stack or invest early in robust inter-process communication infrastructure such as gRPC or message queues rather than relying on stdio streams. The current implementation works for the MVP but would require significant refactoring for production deployment where reliability and observability become critical. 
Technical Lessons: LLM Prompt Engineering: Crafting effective prompts for the Gemini API to generate reliable outputs required substantial iteration and revealed important lessons about structured generation. Early versions of the email drafting prompt produced outputs that mixed Markdown formatting, included preambles like "As an expert lawyer, I would say...", and occasionally hallucinated policy section numbers that did not exist in the provided context. The team learned that prompts must be extremely explicit about format requirements, must include strong negative examples of what not to generate, and must repeatedly emphasize the need to only use information explicitly present in the provided context. The final prompts include instructions like "CRITICAL: You must ONLY use facts and policy details explicitly present in the Context section below. Do NOT invent policy section numbers". Even with these precautions, the system occasionally generates plausible-sounding but incorrect citations, highlighting that LLM outputs require human reviewwhich fortunately aligns with the user-centered design principle of maintaining approval gates. Future work should explore more structured output formats or potentially fine-tuning models on high-quality appeal examples to improve generation reliability. 
User Research Lessons: The Importance of Going Deep: The user research process taught the team that superficial user interviews would have missed the critical insights that shaped the product. The initial interview script focused primarily on asking users about their frustrations with the appeals process. However, the most valuable insights emerged when the team probed deeper into specific stories and asked users to walk through their experiences step-by-step. Jennifer's revelation about how working at a medical clinic taught her "not just to take health insurance and the bills at face value" only emerged through follow-up questions about what distinguished her successful appeal from others who gave up. Mike's knowledge about requesting itemized bills came out when the interviewer asked specifically about his first steps rather than just about frustrations. The lesson here is that structured interview scripts provide valuable consistency, but the true insights come from interviewer skill in recognizing interesting threads and pulling on them through unscripted follow-up questions. Future projects should train interviewers to recognize when a user's casual mention might represent a critical insight worth exploring and should allocate sufficient interview time to go deep rather than covering many surface-level questions. 
User Research Lessons: Validating Assumptions About Automation: Perhaps the most important user research lesson was that the team's initial assumptions about what users wanted were partially wrong. The team assumed that users would want maximum automation to minimize their effort, and that the primary value proposition was saving time. While time-saving proved important, the research revealed that users' primary need was not automation but empowerment through education and maintained control. Andy's rejection of a fully automated system and Jennifer's insistence on review capabilities before sending anything fundamentally reshaped the product from an automation tool to an educational empowerment platform. This lesson reinforces the importance of conducting user research before building, rather than building first and validating later. Had the team proceeded with the initial automation-focused vision without conducting interviews, they would have built a product that users fundamentally mistrusted and refused to adopt. The pivot to transparency and control gates represents the difference between a product that technically works and one that users will actually use. 
Product Strategy Lessons: Scope Management and MVP Definition: Defining the MVP scope required careful judgment about which features were truly essential versus which were important but could be deferred. The team initially wanted to include formal multi-stage appeal management, automated deadline tracking with notifications, and integration with state insurance commissioner portals. User research revealed that over fifty percent of denials in the interview sample were resolved through simple claims corrections via email rather than formal appeals, which validated the decision to focus the MVP on email-based correspondence. However, the team struggled with where to draw the lineshould the MVP include policy document extraction, or could users manually enter plan information? Should it support multiple covered individuals per plan, or just focus on self-coverage? The final decisions to include insurance plan management and covered individual specification proved correct, as testing revealed that users like Mike who manage multiple family members' care found the plan reuse functionality essential. The lesson is that MVP scope should be defined not by arbitrary feature counts but by identifying the minimum set of capabilities that delivers a complete, valuable user experience for the target use case. Features that enable workflow completion for real scenarios are essential even if they add complexity, while features that extend into adjacent use cases can safely be deferred. 
Team Collaboration Lessons: Documentation as Communication: Throughout development, the team learned that comprehensive documentation serves not just as a record but as an essential collaboration tool. The technical_overview.md document became the shared reference point for frontend and backend developers to coordinate on API contracts and data structures. When team members encountered confusion about how the case status workflow should function, referring to the documented state machine in the design specifications resolved ambiguity more effectively than verbal discussions. However, the team also learned that documentation requires active maintenanceseveral times during development, the codebase evolved beyond what the documentation described, leading to confusion when new team members tried to understand the system by reading outdated docs (noted by "(not updated)" markers in design.md). Future projects should establish a practice of updating documentation as part of the code review process, perhaps using automated checks to identify when code changes affect documented APIs or workflows. The documentation should be treated as a living artifact that evolves with the codebase rather than a one-time deliverable produced at project start. 
Process Lessons: The Value of Iterative Development with User Feedback: The team's approach of building core functionality, testing it with realistic scenarios, and then refining based on observations proved more effective than attempting to design a perfect system upfront. The pivot from user personal email sending to agent-based email management emerged from hands-on experience with OAuth integration and recognizing the privacy implications that the initial design created. The decision to separate insurance plan management from case creation came from testing the workflow with a scenario where a user needed to create multiple cases for the same plan. These pivots would not have emerged from design documents alonethey required actually building and using the system to discover where the user experience broke down. The lesson is that in complex workflow applications, iterative development with frequent reality checks against actual usage patterns is essential. Design documents provide direction, but the path to a truly usable product involves responsive adaptation based on what you learn by building and testing. 
Future Work 
PolicyPilot's current implementation establishes a strong foundation for empowering patients to challenge insurance denials, but several important directions for future development would significantly expand the system's impact and capabilities. 
Formal Multi-Stage Appeal Management: The most important functional expansion would be supporting the complete formal appeals process beyond initial email outreach. While user research revealed that many denials can be resolved through claims corrections via direct correspondence, some cases require escalation through internal appeals, external reviews, and potentially regulatory complaints. A future version should implement structured tracking of appeal stages, automatically identify the appropriate next level based on denial responses, generate formal appeal letters that comply with state-specific regulatory requirements, track deadlines for each appeal stage with notifications to users, provide templates for external review requests to independent medical reviewers, and offer guidance on filing complaints with state insurance commissioners when internal processes fail. This expansion would require substantial research into state-specific regulations governing appeals processes, as requirements vary significantly across states and insurance types (Medicare, Medicaid, ACA marketplace, employer-sponsored). The system would need to maintain a database of state-specific deadlines, required forms, and submission procedures. However, implementing this would transform PolicyPilot from a tool for initial outreach into a comprehensive appeals management platform capable of supporting users through the complete journey from initial denial to final resolution. 
Proactive Policy Analysis and Coverage Verification: Rather than waiting for denials to occur, future versions could help users understand their coverage before seeking care. The system could allow users to describe planned procedures or treatments and receive proactive analysis of whether their policy covers the service, identify potential coverage limitations or requirements like prior authorization, estimate out-of-pocket costs based on policy terms, and generate prior authorization request letters when needed. This proactive capability would shift PolicyPilot from a reactive appeals tool to a comprehensive insurance navigation platform. The RAG pipeline already possesses the core capability to query policy documents for coverage informationextending it to proactive queries would primarily require user interface design for describing planned care and presenting coverage analysis in an actionable format. This direction aligns with user research findings that patients want to avoid surprises and understand their coverage before receiving care. 
Integration with Healthcare Provider Systems: A transformative but complex future direction would be integrating PolicyPilot with electronic health records and provider billing systems. Currently, users must manually upload denial letters and medical bills. Automatic ingestion of denial letters directly from insurance company emails or healthcare provider portals, integration with EHR systems to automatically include relevant medical records when building appeals, coordination with provider billing departments to ensure appeals include necessary documentation, and direct submission of appeals through provider advocacy offices where users prefer professional representation would dramatically reduce user effort. This integration would require partnerships with EHR vendors, compliance with HIPAA Business Associate Agreement requirements for handling protected health information, and significant infrastructure investment. However, the potential impact is substantialreducing the barrier to appealing from "gathering documents and writing letters" to "approving a pre-drafted appeal with automatically assembled evidence" would likely increase appeal rates significantly. 
Machine Learning for Outcome Prediction: With sufficient historical data about appeals and their outcomes, future versions could employ machine learning to predict appeal success likelihood and recommend strategies. The system could analyze patterns in successful versus unsuccessful appeals, identify which policy arguments prove most effective with specific insurers, recommend whether to pursue an appeal or accept a denial based on historical success rates, and suggest optimal timing and escalation strategies. Building this capability would require collecting structured outcome data from PolicyPilot users about whether their appeals succeeded, ideally with access to broader datasets about insurance appeal outcomes across the industry (which presents data availability challenges), and careful attention to avoid algorithmic bias that might disadvantage certain patient populations. The ethical implications of such prediction systems require thoughtful considerationusers should understand that predictions are probabilistic rather than deterministic, and the system must avoid discouraging users from pursuing legitimate appeals even when historical success rates are modest. 
Automated Policy Retrieval: Current implementation requires users to manually locate and upload their insurance policy documents, which user research identified as a significant barrier. Future development should explore automated retrieval through screen scraping insurer websites using user-provided credentials (with appropriate security measures), integration with insurer APIs where partnerships can be established, optical character recognition for insurance cards to auto-populate insurer and policy information, and conversational interfaces that guide users through locating policies when automation fails. This feature was consciously deferred from the MVP due to the complexity of building integrations with dozens of diverse insurer portals. However, it represents one of the highest-impact usability improvements possibleeliminating the policy upload step would remove a major friction point in the user workflow. 
Collaborative Features and Community Knowledge: PolicyPilot could evolve from an individual tool into a platform with community-powered knowledge sharing. Future versions might allow users who successfully appealed specific denial types to share their strategies anonymously, create a community-contributed library of effective appeal arguments for common denial scenarios, enable users to see aggregated statistics about which insurers deny which services most frequently, and provide forums where users can discuss experiences and strategies. This community dimension would need careful moderation to ensure privacy protection and prevent misinformation, but could provide powerful peer support that user research suggested many patients lack. The system could use natural language processing to identify common patterns across user appeals and surface best practices automatically. 
Enhanced Email Analysis and Response Strategy: The current email analysis capability identifies weaknesses in insurance company responses but could be substantially enhanced. Future development should implement more sophisticated argument mapping to track how specific policy citations evolve through email exchanges, sentiment analysis to detect when insurance adjusters use delaying tactics or non-responsive arguments, automated detection of procedural violations by insurers (such as missing deadlines or failing to provide required explanations), and strategic recommendations about when to escalate beyond email correspondence to formal channels. These enhancements would make PolicyPilot more effective at navigating the complex negotiation dynamics that characterize insurance appeals. 
Accessibility and Language Support: Expanding accessibility would significantly broaden PolicyPilot's impact. Future work should include comprehensive Spanish language support throughout the interface and for document analysis (serving the substantial Spanish-speaking population in the United States who face particular barriers in healthcare navigation), text-to-speech and screen reader optimization for users with visual impairments, simplified language modes for users with limited health literacy, and mobile-optimized interfaces given that lower-income populations disproportionately access internet services via smartphones. User research identified that lower-income households and limited English proficiency populations face the greatest barriers to successful appeals, making these accessibility enhancements particularly important for health equity impact. 
Integration with Legal Services and Patient Advocacy Organizations: For cases that require professional legal assistance, future versions could facilitate referrals and collaboration. The system could identify cases where professional representation is likely needed based on complexity or stakes involved, provide structured case summaries that attorneys can review efficiently, integrate with pro bono legal services for low-income patients, and partner with patient advocacy organizations to provide casework support for complex situations. This would create a pathway for PolicyPilot to serve as a triage systemhandling straightforward cases autonomously while connecting users to human experts when needed. 
Performance Monitoring and Continuous Improvement: Future deployments should implement comprehensive analytics to track system performance and guide improvements. Key metrics should include appeal success rates compared to baseline population rates, time savings achieved by users, common failure modes where the system produces poor analysis or draft quality, user satisfaction and net promoter scores, and demographic analysis to identify whether the system serves all populations equitably. This data-driven approach to continuous improvement would ensure PolicyPilot evolves based on real-world performance rather than assumptions. The team should establish regular review cycles where analytics inform prioritization of enhancements and refinements. 
The future development roadmap should be guided by continued user research and real-world deployment feedback. While the directions outlined above all represent valuable enhancements, prioritization should be driven by which improvements deliver the greatest impact for users' ability to successfully challenge denials and obtain the coverage they deserve. The current foundation provides a solid platform for this evolution, and the modular architecture facilitates adding capabilities incrementally. 